{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f880c34",
   "metadata": {
    "id": "0f880c34"
   },
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218af616",
   "metadata": {
    "id": "218af616"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "# IOU 계산\n",
    "def intersection_over_union(boxes_preds, boxes_labels, box_format=\"midpoint\"):\n",
    "    \"\"\"\n",
    "    Calculates intersection over union\n",
    "\n",
    "    Parameters:\n",
    "        boxes_preds (tensor): Predictions of Bounding Boxes (BATCH_SIZE, 4)\n",
    "        boxes_labels (tensor): Correct labels of Bounding Boxes (BATCH_SIZE, 4)\n",
    "        box_format (str): midpoint/corners, if boxes (x,y,w,h) or (x1,y1,x2,y2)\n",
    "\n",
    "    Returns:\n",
    "        tensor: Intersection over union for all examples\n",
    "    \"\"\"\n",
    "\n",
    "    if box_format == \"midpoint\":\n",
    "        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
    "        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
    "        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
    "        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
    "        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
    "        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
    "        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
    "        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
    "\n",
    "    if box_format == \"corners\":\n",
    "        box1_x1 = boxes_preds[..., 0:1]\n",
    "        box1_y1 = boxes_preds[..., 1:2]\n",
    "        box1_x2 = boxes_preds[..., 2:3]\n",
    "        box1_y2 = boxes_preds[..., 3:4]  # (N, 1)\n",
    "        box2_x1 = boxes_labels[..., 0:1]\n",
    "        box2_y1 = boxes_labels[..., 1:2]\n",
    "        box2_x2 = boxes_labels[..., 2:3]\n",
    "        box2_y2 = boxes_labels[..., 3:4]\n",
    "\n",
    "    x1 = torch.max(box1_x1, box2_x1)\n",
    "    y1 = torch.max(box1_y1, box2_y1)\n",
    "    x2 = torch.min(box1_x2, box2_x2)\n",
    "    y2 = torch.min(box1_y2, box2_y2)\n",
    "\n",
    "    # .clamp(0) is for the case when they do not intersect\n",
    "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
    "\n",
    "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
    "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
    "\n",
    "    return intersection / (box1_area + box2_area - intersection + 1e-6)\n",
    "\n",
    "\n",
    "# NMS 계산\n",
    "def non_max_suppression(bboxes, iou_threshold, threshold, box_format=\"midpoint\"):\n",
    "    \"\"\"\n",
    "    Does Non Max Suppression given bboxes\n",
    "\n",
    "    Parameters:\n",
    "        bboxes (list): list of lists containing all bboxes with each bboxes\n",
    "        specified as [class_pred, prob_score, x1, y1, x2, y2]\n",
    "        iou_threshold (float): threshold where predicted bboxes is correct\n",
    "        threshold (float): threshold to remove predicted bboxes (independent of IoU)\n",
    "        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n",
    "\n",
    "    Returns:\n",
    "        list: bboxes after performing NMS given a specific IoU threshold\n",
    "    \"\"\"\n",
    "\n",
    "    assert type(bboxes) == list\n",
    "\n",
    "    bboxes = [box for box in bboxes if box[1] > threshold]\n",
    "    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
    "    bboxes_after_nms = []\n",
    "\n",
    "    while bboxes:\n",
    "        chosen_box = bboxes.pop(0)\n",
    "\n",
    "        # 다른 클래스의 박스와 특정 threshold 이하의 박스\n",
    "        # 다시말해서, 같은 클래스의 특정 threshold 이상의 박스를 없앤다.\n",
    "        bboxes = [\n",
    "            box\n",
    "            for box in bboxes\n",
    "            if box[0] != chosen_box[0]\n",
    "            or intersection_over_union(\n",
    "                torch.tensor(chosen_box[2:]),\n",
    "                torch.tensor(box[2:]),\n",
    "                box_format=box_format,\n",
    "            )\n",
    "            < iou_threshold\n",
    "        ]\n",
    "\n",
    "        bboxes_after_nms.append(chosen_box)\n",
    "\n",
    "    return bboxes_after_nms\n",
    "\n",
    "\n",
    "# MAP 계산\n",
    "def mean_average_precision(\n",
    "    pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", num_classes=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates mean average precision\n",
    "\n",
    "    Parameters:\n",
    "        pred_boxes (list): list of lists containing all bboxes with each bboxes\n",
    "        specified as [train_idx, class_prediction, prob_score, x1, y1, x2, y2]\n",
    "        true_boxes (list): Similar as pred_boxes except all the correct ones\n",
    "        iou_threshold (float): threshold where predicted bboxes is correct\n",
    "        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n",
    "        num_classes (int): number of classes\n",
    "\n",
    "    Returns:\n",
    "        float: mAP value across all classes given a specific IoU threshold\n",
    "    \"\"\"\n",
    "\n",
    "    # list storing all AP for respective classes\n",
    "    average_precisions = []\n",
    "\n",
    "    # used for numerical stability later on\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        detections = []\n",
    "        ground_truths = []\n",
    "\n",
    "        # Go through all predictions and targets,\n",
    "        # and only add the ones that belong to the\n",
    "        # current class c\n",
    "        for detection in pred_boxes:\n",
    "            if detection[1] == c:\n",
    "                detections.append(detection)\n",
    "\n",
    "        for true_box in true_boxes:\n",
    "            if true_box[1] == c:\n",
    "                ground_truths.append(true_box)\n",
    "\n",
    "        # find the amount of bboxes for each training example\n",
    "        # Counter here finds how many ground truth bboxes we get\n",
    "        # for each training example, so let's say img 0 has 3,\n",
    "        # img 1 has 5 then we will obtain a dictionary with:\n",
    "        # amount_bboxes = {0:3, 1:5}\n",
    "        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
    "\n",
    "        # We then go through each key, val in this dictionary\n",
    "        # and convert to the following (w.r.t same example):\n",
    "        # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n",
    "        for key, val in amount_bboxes.items():\n",
    "            amount_bboxes[key] = torch.zeros(val)\n",
    "\n",
    "        # sort by box probabilities which is index 2\n",
    "        detections.sort(key=lambda x: x[2], reverse=True)\n",
    "        TP = torch.zeros((len(detections)))\n",
    "        FP = torch.zeros((len(detections)))\n",
    "        total_true_bboxes = len(ground_truths)\n",
    "\n",
    "        # If none exists for this class then we can safely skip\n",
    "        if total_true_bboxes == 0:\n",
    "            continue\n",
    "\n",
    "        for detection_idx, detection in enumerate(detections):\n",
    "            # Only take out the ground_truths that have the same\n",
    "            # training idx as detection\n",
    "            ground_truth_img = [\n",
    "                bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
    "            ]\n",
    "\n",
    "            num_gts = len(ground_truth_img)\n",
    "            best_iou = 0\n",
    "\n",
    "            for idx, gt in enumerate(ground_truth_img):\n",
    "                iou = intersection_over_union(\n",
    "                    torch.tensor(detection[3:]),\n",
    "                    torch.tensor(gt[3:]),\n",
    "                    box_format=box_format,\n",
    "                )\n",
    "\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_gt_idx = idx\n",
    "\n",
    "            if best_iou > iou_threshold:\n",
    "                # only detect ground truth detection once\n",
    "                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
    "                    # true positive and add this bounding box to seen\n",
    "                    TP[detection_idx] = 1\n",
    "                    amount_bboxes[detection[0]][best_gt_idx] = 1\n",
    "                else:\n",
    "                    FP[detection_idx] = 1\n",
    "\n",
    "            # if IOU is lower then the detection is a false positive\n",
    "            else:\n",
    "                FP[detection_idx] = 1\n",
    "\n",
    "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
    "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
    "        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
    "        precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n",
    "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
    "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
    "        # torch.trapz for numerical integration\n",
    "        average_precisions.append(torch.trapz(precisions, recalls))\n",
    "\n",
    "    return sum(average_precisions) / len(average_precisions)\n",
    "\n",
    "\n",
    "# 이미지의 예측된 box와 ground truth box들 얻는 함수\n",
    "def get_bboxes(\n",
    "    loader,\n",
    "    model,\n",
    "    iou_threshold,\n",
    "    threshold,\n",
    "    pred_format=\"cells\",\n",
    "    box_format=\"midpoint\",\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    all_pred_boxes = []\n",
    "    all_true_boxes = []\n",
    "\n",
    "    # make sure model is in eval before get bboxes\n",
    "    model.eval()\n",
    "    train_idx = 0\n",
    "\n",
    "    for batch_idx, (x, labels) in enumerate(loader):\n",
    "        x = x.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = model(x)\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        true_bboxes = cellboxes_to_boxes(labels)\n",
    "        bboxes = cellboxes_to_boxes(predictions)\n",
    "\n",
    "        for idx in range(batch_size):\n",
    "            nms_boxes = non_max_suppression(\n",
    "                bboxes[idx],\n",
    "                iou_threshold=iou_threshold,\n",
    "                threshold=threshold,\n",
    "                box_format=box_format,\n",
    "            )\n",
    "\n",
    "            for nms_box in nms_boxes:\n",
    "                all_pred_boxes.append([train_idx] + nms_box)\n",
    "\n",
    "            for box in true_bboxes[idx]:\n",
    "                # many will get converted to 0 pred\n",
    "                if box[1] > threshold:\n",
    "                    all_true_boxes.append([train_idx] + box)\n",
    "\n",
    "            train_idx += 1\n",
    "\n",
    "    model.train()\n",
    "    return all_pred_boxes, all_true_boxes\n",
    "\n",
    "\n",
    "def convert_cellboxes(predictions, S=7):\n",
    "    \"\"\"\n",
    "    Converts bounding boxes output from Yolo with\n",
    "    an image split size of S into entire image ratios\n",
    "    rather than relative to cell ratios. Tried to do this\n",
    "    vectorized, but this resulted in quite difficult to read\n",
    "    code... Use as a black box? Or implement a more intuitive,\n",
    "    using 2 for loops iterating range(S) and convert them one\n",
    "    by one, resulting in a slower but more readable implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    predictions = predictions.to(\"cpu\")\n",
    "    batch_size = predictions.shape[0]\n",
    "    predictions = predictions.reshape(batch_size, 7, 7, 20)\n",
    "    bboxes1 = predictions[..., 11:15]\n",
    "    bboxes2 = predictions[..., 16:20]\n",
    "    scores = torch.cat(\n",
    "        (predictions[..., 10].unsqueeze(0), predictions[..., 15].unsqueeze(0)), dim=0\n",
    "    )\n",
    "    best_box = scores.argmax(0).unsqueeze(-1)\n",
    "    best_boxes = bboxes1 * (1 - best_box) + best_box * bboxes2\n",
    "    cell_indices = torch.arange(7).repeat(batch_size, 7, 1).unsqueeze(-1)\n",
    "    x = 1 / S * (best_boxes[..., :1] + cell_indices)\n",
    "    y = 1 / S * (best_boxes[..., 1:2] + cell_indices.permute(0, 2, 1, 3))\n",
    "    w_y = 1 / S * best_boxes[..., 2:4]\n",
    "    converted_bboxes = torch.cat((x, y, w_y), dim=-1)\n",
    "    predicted_class = predictions[..., :10].argmax(-1).unsqueeze(-1)\n",
    "    best_confidence = torch.max(predictions[..., 10], predictions[..., 15]).unsqueeze(\n",
    "        -1\n",
    "    )\n",
    "    converted_preds = torch.cat(\n",
    "        (predicted_class, best_confidence, converted_bboxes), dim=-1\n",
    "    )\n",
    "\n",
    "    return converted_preds\n",
    "\n",
    "\n",
    "def cellboxes_to_boxes(out, S=7):\n",
    "    converted_pred = convert_cellboxes(out).reshape(out.shape[0], S * S, -1)\n",
    "    converted_pred[..., 0] = converted_pred[..., 0].long()\n",
    "    all_bboxes = []\n",
    "\n",
    "    for ex_idx in range(out.shape[0]):\n",
    "        bboxes = []\n",
    "\n",
    "        for bbox_idx in range(S * S):\n",
    "            bboxes.append([x.item() for x in converted_pred[ex_idx, bbox_idx, :]])\n",
    "        all_bboxes.append(bboxes)\n",
    "\n",
    "    return all_bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cd32f6",
   "metadata": {
    "id": "58cd32f6"
   },
   "source": [
    "### Custom 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3de89f5",
   "metadata": {
    "id": "f3de89f5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, annotation, data_dir, S=7, B=2, C=10, transforms=None):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        # coco annotation 불러오기 (coco API)\n",
    "        self.coco = COCO(annotation)\n",
    "\n",
    "        # S x S grid 영역\n",
    "        self.S = S\n",
    "\n",
    "        # 각 그리드별 bounding box 개수\n",
    "        self.B = B\n",
    "\n",
    "        # class num\n",
    "        self.C = C\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.coco.getImgIds())\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # 이미지 아이디 가져오기\n",
    "        image_id = self.coco.getImgIds(imgIds=index)\n",
    "\n",
    "        # 이미지 정보 가져오기\n",
    "        image_info = self.coco.loadImgs(image_id)[0]\n",
    "\n",
    "        # 이미지 로드\n",
    "        img_path = os.path.join(self.data_dir, image_info[\"file_name\"])\n",
    "        image = Image.open(img_path)\n",
    "\n",
    "        # 어노테이션 파일 로드\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=image_info[\"id\"])\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "        # 박스 가져오기\n",
    "        bbox = np.array([x[\"bbox\"] for x in anns])\n",
    "\n",
    "        # 레이블 가져오기\n",
    "        labels = np.array([x[\"category_id\"] for x in anns])\n",
    "\n",
    "        # 박스 단위를 0~1로 조정\n",
    "        boxes = []\n",
    "        for box, label in zip(bbox, labels):\n",
    "            boxes.append(\n",
    "                [\n",
    "                    label,\n",
    "                    (box[0] + (box[2] / 2)) / 1024,\n",
    "                    (box[1] + (box[3] / 2)) / 1024,\n",
    "                    (box[2]) / 1024,\n",
    "                    (box[3]) / 1024,\n",
    "                ]\n",
    "            )  # (x_mid, y_mid , width, height)\n",
    "\n",
    "        boxes = torch.tensor(boxes)\n",
    "\n",
    "        if self.transforms:\n",
    "            image, boxes = self.transforms(image, boxes)\n",
    "\n",
    "        # 그리드 단위로 변환\n",
    "        label_matrix = torch.zeros((self.S, self.S, self.C + 5 * self.B))\n",
    "        for box in boxes:\n",
    "            class_label, x, y, width, height = box.tolist()\n",
    "            class_label = int(class_label)\n",
    "\n",
    "            # i,j 는 박스가 위치하는 row, column을 의미\n",
    "            i, j = int(self.S * y), int(self.S * x)\n",
    "            x_cell, y_cell = self.S * x - j, self.S * y - i\n",
    "\n",
    "            \"\"\"\n",
    "            Calculating the width and height of cell of bounding box,\n",
    "            relative to the cell is done by the following, with\n",
    "            width as the example:\n",
    "            \n",
    "            width_pixels = (width*self.image_width)\n",
    "            cell_pixels = (self.image_width)\n",
    "            \n",
    "            Then to find the width relative to the cell is simply:\n",
    "            width_pixels/cell_pixels, simplification leads to the\n",
    "            formulas below.\n",
    "            \"\"\"\n",
    "            # 높이, 너비 그리드\n",
    "            width_cell, height_cell = (\n",
    "                width * self.S,\n",
    "                height * self.S,\n",
    "            )\n",
    "\n",
    "            # If no object already found for specific cell i,j\n",
    "            # Note: This means we restrict to ONE object\n",
    "            # per cell!\n",
    "            # 각 그리드당 박스 개수 하나로 제한\n",
    "            if label_matrix[i, j, self.C] == 0:\n",
    "                # 해당 그리드에 박스가 존재한다는 표시\n",
    "                label_matrix[i, j, self.C] = 1\n",
    "\n",
    "                # 박스 좌표 (그리드 단위)\n",
    "                box_coordinates = torch.tensor(\n",
    "                    [x_cell, y_cell, width_cell, height_cell]\n",
    "                )\n",
    "\n",
    "                label_matrix[i, j, self.C + 1 : self.C + 5] = box_coordinates\n",
    "\n",
    "                # class label을 one-hot encoding으로 처리\n",
    "                label_matrix[i, j, class_label] = 1\n",
    "\n",
    "        return image, label_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d10387",
   "metadata": {
    "id": "a3d10387"
   },
   "source": [
    "### YOLO Model 정의\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4831a0",
   "metadata": {
    "id": "5b4831a0"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of Yolo (v1) architecture\n",
    "with slight modification with added BatchNorm.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# yolo architecture config\n",
    "# (kerenl_size, filters, stride, padding)\n",
    "# M은 2x2 maxpooling\n",
    "architecture_config = [\n",
    "    (7, 64, 2, 3),\n",
    "    \"M\",\n",
    "    (3, 192, 1, 1),\n",
    "    \"M\",\n",
    "    (1, 128, 1, 0),\n",
    "    (3, 256, 1, 1),\n",
    "    (1, 256, 1, 0),\n",
    "    (3, 512, 1, 1),\n",
    "    \"M\",\n",
    "    [(1, 256, 1, 0), (3, 512, 1, 1), 4],\n",
    "    (1, 512, 1, 0),\n",
    "    (3, 1024, 1, 1),\n",
    "    \"M\",\n",
    "    [(1, 512, 1, 0), (3, 1024, 1, 1), 2],\n",
    "    (3, 1024, 1, 1),\n",
    "    (3, 1024, 2, 1),\n",
    "    (3, 1024, 1, 1),\n",
    "    (3, 1024, 1, 1),\n",
    "]\n",
    "\n",
    "# convolution layer, batchnormalization, leakyrelu로 구성된 하나의 Block\n",
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        self.leakyrelu = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.leakyrelu(self.batchnorm(self.conv(x)))\n",
    "\n",
    "\n",
    "# 실제 사용될 model\n",
    "class Yolov1(nn.Module):\n",
    "    def __init__(self, in_channels=3, **kwargs):\n",
    "        super(Yolov1, self).__init__()\n",
    "        self.architecture = architecture_config\n",
    "        self.in_channels = in_channels\n",
    "        self.darknet = self._create_conv_layers(self.architecture)\n",
    "        self.fcs = self._create_fcs(**kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.darknet(x)\n",
    "        return self.fcs(torch.flatten(x, start_dim=1))\n",
    "\n",
    "    # architecture config에 따라서 darknet(backbone) 생성\n",
    "    def _create_conv_layers(self, architecture):\n",
    "        layers = []\n",
    "        in_channels = self.in_channels\n",
    "\n",
    "        for x in architecture:\n",
    "            # convolution block 생성\n",
    "            if type(x) == tuple:\n",
    "                layers += [\n",
    "                    CNNBlock(\n",
    "                        in_channels,\n",
    "                        x[1],\n",
    "                        kernel_size=x[0],\n",
    "                        stride=x[2],\n",
    "                        padding=x[3],\n",
    "                    )\n",
    "                ]\n",
    "                in_channels = x[1]\n",
    "\n",
    "            # max pooling\n",
    "            elif type(x) == str:\n",
    "                layers += [nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))]\n",
    "\n",
    "            # deeper convolution block\n",
    "            elif type(x) == list:\n",
    "                conv1 = x[0]\n",
    "                conv2 = x[1]\n",
    "                num_repeats = x[2]\n",
    "\n",
    "                for _ in range(num_repeats):\n",
    "                    layers += [\n",
    "                        CNNBlock(\n",
    "                            in_channels,\n",
    "                            conv1[1],\n",
    "                            kernel_size=conv1[0],\n",
    "                            stride=conv1[2],\n",
    "                            padding=conv1[3],\n",
    "                        )\n",
    "                    ]\n",
    "                    layers += [\n",
    "                        CNNBlock(\n",
    "                            conv1[1],\n",
    "                            conv2[1],\n",
    "                            kernel_size=conv2[0],\n",
    "                            stride=conv2[2],\n",
    "                            padding=conv2[3],\n",
    "                        )\n",
    "                    ]\n",
    "                    in_channels = conv2[1]\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    # 그리드 크기, 박스 개수, 클래스 개수에 따라서 Prediction head 생성\n",
    "    def _create_fcs(self, split_size, num_boxes, num_classes):\n",
    "        S, B, C = split_size, num_boxes, num_classes\n",
    "\n",
    "        # In original paper this should be\n",
    "        # nn.Linear(1024*S*S, 4096),\n",
    "        # nn.LeakyReLU(0.1),\n",
    "        # nn.Linear(4096, S*S*(B*5+C))\n",
    "\n",
    "        return nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024 * S * S, 496),\n",
    "            nn.Dropout(0.0),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(496, S * S * (C + B * 5)),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc81e900",
   "metadata": {
    "id": "fc81e900"
   },
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6813cea3",
   "metadata": {
    "id": "6813cea3"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of Yolo Loss Function from the original yolo paper\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# from utils import intersection_over_union\n",
    "\n",
    "\n",
    "class YoloLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Calculate the loss for yolo (v1) model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, S=7, B=2, C=10):\n",
    "        super(YoloLoss, self).__init__()\n",
    "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "        \"\"\"\n",
    "        S is split size of image (in paper 7),\n",
    "        B is number of boxes (in paper 2),\n",
    "        C is number of classes (Custom dataset is 10),\n",
    "        \"\"\"\n",
    "        self.S = S  # 그리드 크기\n",
    "        self.B = B  # bounding box 수\n",
    "        self.C = C  # class 수\n",
    "        self.lambda_noobj = 0.5\n",
    "        self.lambda_coord = 5\n",
    "\n",
    "    def forward(self, predictions, target):\n",
    "        predictions = predictions.reshape(\n",
    "            -1, self.S, self.S, self.C + self.B * 5\n",
    "        )  # 7x7x20 feature map flatten\n",
    "\n",
    "        # 예측한 2개의 bounding box의 IoU 계산\n",
    "        # 첫번째 bounding box와 target과 iou 계산\n",
    "        iou_b1 = intersection_over_union(\n",
    "            predictions[..., 11:15], target[..., 11:15]\n",
    "        )  # [..., 11:15] 첫번째 bounding box\n",
    "        # 두번째 bounding box와 target과 iou 계산\n",
    "        iou_b2 = intersection_over_union(\n",
    "            predictions[..., 16:20], target[..., 11:15]\n",
    "        )  # [..., 16:20] 두번째 bounding box\n",
    "        ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n",
    "\n",
    "        iou_maxes, bestbox = torch.max(ious, dim=0)  # bounding box 두개 중 더 큰 IoU를 가진 박스\n",
    "        exists_box = target[..., 10].unsqueeze(\n",
    "            3\n",
    "        )  # 해당 grid cell에 ground-truth가 존재하는지 여부 (1 : 존재, 0 : 존재x)\n",
    "\n",
    "        # ======================== #\n",
    "        #     Localization Loss    #\n",
    "        # ======================== #\n",
    "\n",
    "        # box_predictions : IoU 더 큰 값의 bounding box\n",
    "        box_predictions = exists_box * (  # ground-truth가 존재하면 예측\n",
    "            (\n",
    "                bestbox * predictions[..., 16:20]  # IoU가 더 큰 박스가 두번째 박스일 때\n",
    "                + (1 - bestbox) * predictions[..., 11:15]  # IoU가 더 큰 박스가 첫번째 박스일 때\n",
    "            )\n",
    "        )\n",
    "\n",
    "        box_targets = exists_box * target[..., 11:15]\n",
    "\n",
    "        # width, height 루트 씌우기\n",
    "        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(\n",
    "            torch.abs(box_predictions[..., 2:4] + 1e-6)\n",
    "        )\n",
    "        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n",
    "\n",
    "        # MSE loss\n",
    "        box_loss = self.mse(\n",
    "            torch.flatten(box_predictions, end_dim=-2),\n",
    "            torch.flatten(box_targets, end_dim=-2),\n",
    "        )\n",
    "\n",
    "        # ======================== #\n",
    "        #      Confidence Loss     #\n",
    "        # ======================== #\n",
    "\n",
    "        # confidence loss는 object가 있을 때, 없을 때 나눠서 계산 (exists_box: object 존재 유무)\n",
    "\n",
    "        ### For Object Loss ###\n",
    "\n",
    "        # pred_box : IoU가 큰 box의 confidence score\n",
    "        pred_box = (\n",
    "            bestbox * predictions[..., 15:16] + (1 - bestbox) * predictions[..., 10:11]\n",
    "        )\n",
    "        # MSE Loss\n",
    "        object_loss = self.mse(\n",
    "            torch.flatten(exists_box * pred_box),\n",
    "            torch.flatten(exists_box * target[..., 10:11]),\n",
    "        )\n",
    "\n",
    "        ### For No Object Loss ###\n",
    "        # object가 없을 때는 두개의 bounding box 모두 계산\n",
    "\n",
    "        # 첫번째 bounding box의 MSE loss\n",
    "        no_object_loss = self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., 10:11], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * target[..., 10:11], start_dim=1),\n",
    "        )\n",
    "        # 두번째 bounding box의 MSE loss\n",
    "        no_object_loss += self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., 15:16], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * target[..., 10:11], start_dim=1),\n",
    "        )\n",
    "\n",
    "        # ======================== #\n",
    "        #    Classification Loss   #\n",
    "        # ======================== #\n",
    "\n",
    "        # MSE loss\n",
    "        class_loss = self.mse(\n",
    "            torch.flatten(\n",
    "                exists_box * predictions[..., :10],\n",
    "                end_dim=-2,\n",
    "            ),\n",
    "            torch.flatten(\n",
    "                exists_box * target[..., :10],\n",
    "                end_dim=-2,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # ======================== #\n",
    "        #         Final Loss       #\n",
    "        # ======================== #\n",
    "\n",
    "        loss = (\n",
    "            self.lambda_coord * box_loss  # localization loss\n",
    "            + object_loss  # confidence loss (object 있을 때)\n",
    "            + self.lambda_noobj * no_object_loss  # confidence loss (object 없을 때)\n",
    "            + class_loss  # classification loss\n",
    "        )\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba67b521",
   "metadata": {
    "id": "ba67b521"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6035c5b",
   "metadata": {
    "id": "d6035c5b"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main file for training Yolo model on Custom dataset\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms.functional as FT\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "seed = 123\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Hyperparameters etc.\n",
    "LEARNING_RATE = 2e-5\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "BATCH_SIZE = 16\n",
    "WEIGHT_DECAY = 0\n",
    "EPOCHS = 100\n",
    "NUM_WORKERS = 2\n",
    "PIN_MEMORY = False\n",
    "LOAD_MODEL = False\n",
    "LOAD_MODEL_FILE = \"./yolo.pth\"\n",
    "\n",
    "\n",
    "# 사용할 transform 정의\n",
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img, bboxes):\n",
    "        for t in self.transforms:\n",
    "            img, bboxes = t(img), bboxes\n",
    "\n",
    "        return img, bboxes\n",
    "\n",
    "\n",
    "# transform init\n",
    "transform = Compose(\n",
    "    [\n",
    "        transforms.Resize((448, 448)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# train function\n",
    "def train_fn(train_loader, model, optimizer, scheduler, loss_fn):\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    mean_loss = []\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(loop):\n",
    "        # x: image, y: box 정보\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "        # yolo model output\n",
    "        out = model(x)\n",
    "\n",
    "        # output과 box정보 (ground truth) 를 이용하여 loss 계산\n",
    "        loss = loss_fn(out, y)\n",
    "        mean_loss.append(loss.item())\n",
    "\n",
    "        # loss backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # update progress bar\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    return sum(mean_loss) / len(mean_loss)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # model 생성\n",
    "    model = Yolov1(split_size=7, num_boxes=2, num_classes=10).to(DEVICE)\n",
    "\n",
    "    # Adam optimizer 이용\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "    # scheduler\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(\n",
    "        optimizer, milestones=[40, 70], gamma=0.1\n",
    "    )\n",
    "\n",
    "    # Yolo loss function\n",
    "    loss_fn = YoloLoss()\n",
    "\n",
    "    # pretrained weight 사용할 시에 동작\n",
    "    if LOAD_MODEL:\n",
    "        model.load_state_dict(torch.load(LOAD_MODEL_FILE))\n",
    "\n",
    "    # train dataset 생성\n",
    "    train_dataset = CustomDataset(\n",
    "        \"../../dataset/train.json\",\n",
    "        \"../../dataset\",\n",
    "        transforms=transform,\n",
    "    )\n",
    "\n",
    "    # test dataset 생성\n",
    "    test_dataset = CustomDataset(\n",
    "        \"../../dataset/train.json\",\n",
    "        \"../../dataset\",\n",
    "        transforms=transform,\n",
    "    )\n",
    "    # train dataset loader\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    # test dataset loader\n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        shuffle=False,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    low_loss = 1000\n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "        # model train\n",
    "        loss = train_fn(train_loader, model, optimizer, scheduler, loss_fn)\n",
    "        print(f\"#{epoch+1}: Mean loss was {loss}\")\n",
    "\n",
    "        # checkpoint 저장\n",
    "        if loss < low_loss:\n",
    "            torch.save(model.state_dict(), \"./yolo_s.pth\")\n",
    "            low_loss = loss\n",
    "\n",
    "        # 학습된 model로 test dataset(== train_dataset)의 prediction box와 target box 생성\n",
    "        pred_boxes, target_boxes = get_bboxes(\n",
    "            test_loader, model, iou_threshold=0.5, threshold=0.4\n",
    "        )\n",
    "\n",
    "        # model이 얼마나 정확히 예측하였는지 mAP계산\n",
    "        mean_avg_prec = mean_average_precision(\n",
    "            pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\"\n",
    "        )\n",
    "\n",
    "        # train의 mAP 계산\n",
    "        print(f\"Train mAP: {mean_avg_prec}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de4353f",
   "metadata": {
    "id": "0de4353f"
   },
   "source": [
    "### Reference\n",
    "https://github.com/aladdinpersson/Machine-Learning-Collection/tree/master/ML/Pytorch/object_detection/YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01aab9cd",
   "metadata": {
    "id": "01aab9cd"
   },
   "source": [
    "###**콘텐츠 라이선스**\n",
    "\n",
    "<font color='red'><b>**WARNING**</b></font> : **본 교육 콘텐츠의 지식재산권은 재단법인 네이버커넥트에 귀속됩니다. 본 콘텐츠를 어떠한 경로로든 외부로 유출 및 수정하는 행위를 엄격히 금합니다.** 다만, 비영리적 교육 및 연구활동에 한정되어 사용할 수 있으나 재단의 허락을 받아야 합니다. 이를 위반하는 경우, 관련 법률에 따라 책임을 질 수 있습니다.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f880c34",
   "metadata": {
    "id": "0f880c34"
   },
   "source": [
    "### Utils\n",
    "ğŸ‘‰ mission.NMS(Non Maximum Suppression)\\\n",
    "'ì½”ë“œ ì¶”ê°€' ì— ì½”ë“œë¥¼ ì¶”ê°€í•´ ì£¼ì‹œë©´ ë©ë‹ˆë‹¤! (ì–´ë ¤ìš°ì‹  ê²½ìš° ê°•ì˜ì™€ ì •ë‹µì½”ë“œë¥¼ ì°¸ê³ í•´ì„œ ì¶”ê°€í•´ì£¼ì„¸ìš”~)\n",
    "1. íŠ¹ì •ë°•ìŠ¤ í•œ ê°œ ì„ íƒ\n",
    "2. ê·¸ íŠ¹ì • ë°•ìŠ¤ì™€ ë‹¤ë¥¸ í´ë˜ìŠ¤ëŠ” ë‚¨ê¹€\n",
    "3. ê·¸ íŠ¹ì • ë°•ìŠ¤ì™€ ê°™ì€ í´ë˜ìŠ¤ì§€ë§Œ iouê°€ threshold ì´í•˜ëŠ” ë‚¨ê¹€\n",
    "\n",
    "ë”°ë¼ì„œ ê·¸ íŠ¹ì • ë°•ìŠ¤ì™€ ê°™ì€ í´ë˜ìŠ¤ë©´ì„œ iouê°€ threshold ì´ìƒì¸ ë°•ìŠ¤ë“¤ì„ ì œê±°\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218af616",
   "metadata": {
    "id": "218af616"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "# IOU ê³„ì‚°\n",
    "def intersection_over_union(boxes_preds, boxes_labels, box_format=\"midpoint\"):\n",
    "    \"\"\"\n",
    "    Calculates intersection over union\n",
    "\n",
    "    Parameters:\n",
    "        boxes_preds (tensor): Predictions of Bounding Boxes (BATCH_SIZE, 4)\n",
    "        boxes_labels (tensor): Correct labels of Bounding Boxes (BATCH_SIZE, 4)\n",
    "        box_format (str): midpoint/corners, if boxes (x,y,w,h) or (x1,y1,x2,y2)\n",
    "\n",
    "    Returns:\n",
    "        tensor: Intersection over union for all examples\n",
    "    \"\"\"\n",
    "\n",
    "    if box_format == \"midpoint\":\n",
    "        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
    "        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
    "        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
    "        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
    "        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
    "        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
    "        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
    "        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
    "\n",
    "    if box_format == \"corners\":\n",
    "        box1_x1 = boxes_preds[..., 0:1]\n",
    "        box1_y1 = boxes_preds[..., 1:2]\n",
    "        box1_x2 = boxes_preds[..., 2:3]\n",
    "        box1_y2 = boxes_preds[..., 3:4]  # (N, 1)\n",
    "        box2_x1 = boxes_labels[..., 0:1]\n",
    "        box2_y1 = boxes_labels[..., 1:2]\n",
    "        box2_x2 = boxes_labels[..., 2:3]\n",
    "        box2_y2 = boxes_labels[..., 3:4]\n",
    "\n",
    "    x1 = torch.max(box1_x1, box2_x1)\n",
    "    y1 = torch.max(box1_y1, box2_y1)\n",
    "    x2 = torch.min(box1_x2, box2_x2)\n",
    "    y2 = torch.min(box1_y2, box2_y2)\n",
    "\n",
    "    # .clamp(0) is for the case when they do not intersect\n",
    "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
    "\n",
    "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
    "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
    "\n",
    "    return intersection / (box1_area + box2_area - intersection + 1e-6)\n",
    "\n",
    "\n",
    "# NMS ê³„ì‚°\n",
    "def non_max_suppression(bboxes, iou_threshold, threshold, box_format=\"midpoint\"):\n",
    "    \"\"\"\n",
    "    Does Non Max Suppression given bboxes\n",
    "\n",
    "    Parameters:\n",
    "        bboxes (list): list of lists containing all bboxes with each bboxes\n",
    "        specified as [class_pred, prob_score, x1, y1, x2, y2]\n",
    "        iou_threshold (float): threshold where predicted bboxes is correct\n",
    "        threshold (float): threshold to remove predicted bboxes (independent of IoU)\n",
    "        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n",
    "\n",
    "    Returns:\n",
    "        list: bboxes after performing NMS given a specific IoU threshold\n",
    "    \"\"\"\n",
    "\n",
    "    assert type(bboxes) == list\n",
    "\n",
    "    bboxes = [box for box in bboxes if box[1] > threshold]\n",
    "    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
    "    bboxes_after_nms = []\n",
    "\n",
    "    while bboxes:\n",
    "        chosen_box = \"ì½”ë“œ ì¶”ê°€\"\n",
    "\n",
    "        # ë‹¤ë¥¸ í´ë˜ìŠ¤ì˜ ë°•ìŠ¤ì™€ íŠ¹ì • threshold ì´í•˜ì˜ ë°•ìŠ¤ë¥¼ ë‚¨ê¹€\n",
    "        # ë‹¤ì‹œë§í•´ì„œ, ê°™ì€ í´ë˜ìŠ¤ì˜ íŠ¹ì • threshold ì´ìƒì˜ ë°•ìŠ¤ë¥¼ ì—†ì•¤ë‹¤.\n",
    "        bboxes = [box for box in bboxes if \"ì½”ë“œ ì¶”ê°€\" or \"ì½”ë“œ ì¶”ê°€\"]\n",
    "\n",
    "        bboxes_after_nms.append(chosen_box)\n",
    "\n",
    "    return bboxes_after_nms\n",
    "\n",
    "\n",
    "# MAP ê³„ì‚°\n",
    "def mean_average_precision(\n",
    "    pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", num_classes=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates mean average precision\n",
    "\n",
    "    Parameters:\n",
    "        pred_boxes (list): list of lists containing all bboxes with each bboxes\n",
    "        specified as [train_idx, class_prediction, prob_score, x1, y1, x2, y2]\n",
    "        true_boxes (list): Similar as pred_boxes except all the correct ones\n",
    "        iou_threshold (float): threshold where predicted bboxes is correct\n",
    "        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n",
    "        num_classes (int): number of classes\n",
    "\n",
    "    Returns:\n",
    "        float: mAP value across all classes given a specific IoU threshold\n",
    "    \"\"\"\n",
    "\n",
    "    # list storing all AP for respective classes\n",
    "    average_precisions = []\n",
    "\n",
    "    # used for numerical stability later on\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        detections = []\n",
    "        ground_truths = []\n",
    "\n",
    "        # Go through all predictions and targets,\n",
    "        # and only add the ones that belong to the\n",
    "        # current class c\n",
    "        for detection in pred_boxes:\n",
    "            if detection[1] == c:\n",
    "                detections.append(detection)\n",
    "\n",
    "        for true_box in true_boxes:\n",
    "            if true_box[1] == c:\n",
    "                ground_truths.append(true_box)\n",
    "\n",
    "        # find the amount of bboxes for each training example\n",
    "        # Counter here finds how many ground truth bboxes we get\n",
    "        # for each training example, so let's say img 0 has 3,\n",
    "        # img 1 has 5 then we will obtain a dictionary with:\n",
    "        # amount_bboxes = {0:3, 1:5}\n",
    "        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
    "\n",
    "        # We then go through each key, val in this dictionary\n",
    "        # and convert to the following (w.r.t same example):\n",
    "        # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n",
    "        for key, val in amount_bboxes.items():\n",
    "            amount_bboxes[key] = torch.zeros(val)\n",
    "\n",
    "        # sort by box probabilities which is index 2\n",
    "        detections.sort(key=lambda x: x[2], reverse=True)\n",
    "        TP = torch.zeros((len(detections)))\n",
    "        FP = torch.zeros((len(detections)))\n",
    "        total_true_bboxes = len(ground_truths)\n",
    "\n",
    "        # If none exists for this class then we can safely skip\n",
    "        if total_true_bboxes == 0:\n",
    "            continue\n",
    "\n",
    "        for detection_idx, detection in enumerate(detections):\n",
    "            # Only take out the ground_truths that have the same\n",
    "            # training idx as detection\n",
    "            ground_truth_img = [\n",
    "                bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
    "            ]\n",
    "\n",
    "            num_gts = len(ground_truth_img)\n",
    "            best_iou = 0\n",
    "\n",
    "            for idx, gt in enumerate(ground_truth_img):\n",
    "                iou = intersection_over_union(\n",
    "                    torch.tensor(detection[3:]),\n",
    "                    torch.tensor(gt[3:]),\n",
    "                    box_format=box_format,\n",
    "                )\n",
    "\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_gt_idx = idx\n",
    "\n",
    "            if best_iou > iou_threshold:\n",
    "                # only detect ground truth detection once\n",
    "                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
    "                    # true positive and add this bounding box to seen\n",
    "                    TP[detection_idx] = 1\n",
    "                    amount_bboxes[detection[0]][best_gt_idx] = 1\n",
    "                else:\n",
    "                    FP[detection_idx] = 1\n",
    "\n",
    "            # if IOU is lower then the detection is a false positive\n",
    "            else:\n",
    "                FP[detection_idx] = 1\n",
    "\n",
    "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
    "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
    "        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
    "        precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n",
    "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
    "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
    "        # torch.trapz for numerical integration\n",
    "        average_precisions.append(torch.trapz(precisions, recalls))\n",
    "\n",
    "    return sum(average_precisions) / len(average_precisions)\n",
    "\n",
    "\n",
    "# ì´ë¯¸ì§€ì˜ ì˜ˆì¸¡ëœ boxì™€ ground truth boxë“¤ ì–»ëŠ” í•¨ìˆ˜\n",
    "def get_bboxes(\n",
    "    loader,\n",
    "    model,\n",
    "    iou_threshold,\n",
    "    threshold,\n",
    "    pred_format=\"cells\",\n",
    "    box_format=\"midpoint\",\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    all_pred_boxes = []\n",
    "    all_true_boxes = []\n",
    "\n",
    "    # make sure model is in eval before get bboxes\n",
    "    model.eval()\n",
    "    train_idx = 0\n",
    "\n",
    "    for batch_idx, (x, labels) in enumerate(loader):\n",
    "        x = x.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = model(x)\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        true_bboxes = cellboxes_to_boxes(labels)\n",
    "        bboxes = cellboxes_to_boxes(predictions)\n",
    "\n",
    "        for idx in range(batch_size):\n",
    "            nms_boxes = non_max_suppression(\n",
    "                bboxes[idx],\n",
    "                iou_threshold=iou_threshold,\n",
    "                threshold=threshold,\n",
    "                box_format=box_format,\n",
    "            )\n",
    "\n",
    "            for nms_box in nms_boxes:\n",
    "                all_pred_boxes.append([train_idx] + nms_box)\n",
    "\n",
    "            for box in true_bboxes[idx]:\n",
    "                # many will get converted to 0 pred\n",
    "                if box[1] > threshold:\n",
    "                    all_true_boxes.append([train_idx] + box)\n",
    "\n",
    "            train_idx += 1\n",
    "\n",
    "    model.train()\n",
    "    return all_pred_boxes, all_true_boxes\n",
    "\n",
    "\n",
    "def convert_cellboxes(predictions, S=7):\n",
    "    \"\"\"\n",
    "    Converts bounding boxes output from Yolo with\n",
    "    an image split size of S into entire image ratios\n",
    "    rather than relative to cell ratios. Tried to do this\n",
    "    vectorized, but this resulted in quite difficult to read\n",
    "    code... Use as a black box? Or implement a more intuitive,\n",
    "    using 2 for loops iterating range(S) and convert them one\n",
    "    by one, resulting in a slower but more readable implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    predictions = predictions.to(\"cpu\")\n",
    "    batch_size = predictions.shape[0]\n",
    "    predictions = predictions.reshape(batch_size, 7, 7, 20)\n",
    "    bboxes1 = predictions[..., 11:15]\n",
    "    bboxes2 = predictions[..., 16:20]\n",
    "    scores = torch.cat(\n",
    "        (predictions[..., 10].unsqueeze(0), predictions[..., 15].unsqueeze(0)), dim=0\n",
    "    )\n",
    "    best_box = scores.argmax(0).unsqueeze(-1)\n",
    "    best_boxes = bboxes1 * (1 - best_box) + best_box * bboxes2\n",
    "    cell_indices = torch.arange(7).repeat(batch_size, 7, 1).unsqueeze(-1)\n",
    "    x = 1 / S * (best_boxes[..., :1] + cell_indices)\n",
    "    y = 1 / S * (best_boxes[..., 1:2] + cell_indices.permute(0, 2, 1, 3))\n",
    "    w_y = 1 / S * best_boxes[..., 2:4]\n",
    "    converted_bboxes = torch.cat((x, y, w_y), dim=-1)\n",
    "    predicted_class = predictions[..., :10].argmax(-1).unsqueeze(-1)\n",
    "    best_confidence = torch.max(predictions[..., 10], predictions[..., 15]).unsqueeze(\n",
    "        -1\n",
    "    )\n",
    "    converted_preds = torch.cat(\n",
    "        (predicted_class, best_confidence, converted_bboxes), dim=-1\n",
    "    )\n",
    "\n",
    "    return converted_preds\n",
    "\n",
    "\n",
    "def cellboxes_to_boxes(out, S=7):\n",
    "    converted_pred = convert_cellboxes(out).reshape(out.shape[0], S * S, -1)\n",
    "    converted_pred[..., 0] = converted_pred[..., 0].long()\n",
    "    all_bboxes = []\n",
    "\n",
    "    for ex_idx in range(out.shape[0]):\n",
    "        bboxes = []\n",
    "\n",
    "        for bbox_idx in range(S * S):\n",
    "            bboxes.append([x.item() for x in converted_pred[ex_idx, bbox_idx, :]])\n",
    "        all_bboxes.append(bboxes)\n",
    "\n",
    "    return all_bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cd32f6",
   "metadata": {
    "id": "58cd32f6"
   },
   "source": [
    "### Custom ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3de89f5",
   "metadata": {
    "id": "f3de89f5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, annotation, data_dir, S=7, B=2, C=10, transforms=None):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        # coco annotation ë¶ˆëŸ¬ì˜¤ê¸° (coco API)\n",
    "        self.coco = COCO(annotation)\n",
    "\n",
    "        # S x S grid ì˜ì—­\n",
    "        self.S = S\n",
    "\n",
    "        # ê° ê·¸ë¦¬ë“œë³„ bounding box ê°œìˆ˜\n",
    "        self.B = B\n",
    "\n",
    "        # class num\n",
    "        self.C = C\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.coco.getImgIds())\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # ì´ë¯¸ì§€ ì•„ì´ë”” ê°€ì ¸ì˜¤ê¸°\n",
    "        image_id = self.coco.getImgIds(imgIds=index)\n",
    "\n",
    "        # ì´ë¯¸ì§€ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
    "        image_info = self.coco.loadImgs(image_id)[0]\n",
    "\n",
    "        # ì´ë¯¸ì§€ ë¡œë“œ\n",
    "        img_path = os.path.join(self.data_dir, image_info[\"file_name\"])\n",
    "        image = Image.open(img_path)\n",
    "\n",
    "        # ì–´ë…¸í…Œì´ì…˜ íŒŒì¼ ë¡œë“œ\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=image_info[\"id\"])\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "        # ë°•ìŠ¤ ê°€ì ¸ì˜¤ê¸°\n",
    "        bbox = np.array([x[\"bbox\"] for x in anns])\n",
    "\n",
    "        # ë ˆì´ë¸” ê°€ì ¸ì˜¤ê¸°\n",
    "        labels = np.array([x[\"category_id\"] for x in anns])\n",
    "\n",
    "        # ë°•ìŠ¤ ë‹¨ìœ„ë¥¼ 0~1ë¡œ ì¡°ì •\n",
    "        boxes = []\n",
    "        for box, label in zip(bbox, labels):\n",
    "            boxes.append(\n",
    "                [\n",
    "                    label,\n",
    "                    (box[0] + (box[2] / 2)) / 1024,\n",
    "                    (box[1] + (box[3] / 2)) / 1024,\n",
    "                    (box[2]) / 1024,\n",
    "                    (box[3]) / 1024,\n",
    "                ]\n",
    "            )  # (x_mid, y_mid , width, height)\n",
    "\n",
    "        boxes = torch.tensor(boxes)\n",
    "\n",
    "        if self.transforms:\n",
    "            image, boxes = self.transforms(image, boxes)\n",
    "\n",
    "        # ê·¸ë¦¬ë“œ ë‹¨ìœ„ë¡œ ë³€í™˜\n",
    "        label_matrix = torch.zeros((self.S, self.S, self.C + 5 * self.B))\n",
    "        for box in boxes:\n",
    "            class_label, x, y, width, height = box.tolist()\n",
    "            class_label = int(class_label)\n",
    "\n",
    "            # i,j ëŠ” ë°•ìŠ¤ê°€ ìœ„ì¹˜í•˜ëŠ” row, columnì„ ì˜ë¯¸\n",
    "            i, j = int(self.S * y), int(self.S * x)\n",
    "            x_cell, y_cell = self.S * x - j, self.S * y - i\n",
    "\n",
    "            \"\"\"\n",
    "            Calculating the width and height of cell of bounding box,\n",
    "            relative to the cell is done by the following, with\n",
    "            width as the example:\n",
    "            \n",
    "            width_pixels = (width*self.image_width)\n",
    "            cell_pixels = (self.image_width)\n",
    "            \n",
    "            Then to find the width relative to the cell is simply:\n",
    "            width_pixels/cell_pixels, simplification leads to the\n",
    "            formulas below.\n",
    "            \"\"\"\n",
    "            # ë†’ì´, ë„ˆë¹„ ê·¸ë¦¬ë“œ\n",
    "            width_cell, height_cell = (\n",
    "                width * self.S,\n",
    "                height * self.S,\n",
    "            )\n",
    "\n",
    "            # If no object already found for specific cell i,j\n",
    "            # Note: This means we restrict to ONE object\n",
    "            # per cell!\n",
    "            # ê° ê·¸ë¦¬ë“œë‹¹ ë°•ìŠ¤ ê°œìˆ˜ í•˜ë‚˜ë¡œ ì œí•œ\n",
    "            if label_matrix[i, j, self.C] == 0:\n",
    "                # í•´ë‹¹ ê·¸ë¦¬ë“œì— ë°•ìŠ¤ê°€ ì¡´ì¬í•œë‹¤ëŠ” í‘œì‹œ\n",
    "                label_matrix[i, j, self.C] = 1\n",
    "\n",
    "                # ë°•ìŠ¤ ì¢Œí‘œ (ê·¸ë¦¬ë“œ ë‹¨ìœ„)\n",
    "                box_coordinates = torch.tensor(\n",
    "                    [x_cell, y_cell, width_cell, height_cell]\n",
    "                )\n",
    "\n",
    "                label_matrix[i, j, self.C + 1 : self.C + 5] = box_coordinates\n",
    "\n",
    "                # class labelì„ one-hot encodingìœ¼ë¡œ ì²˜ë¦¬\n",
    "                label_matrix[i, j, class_label] = 1\n",
    "\n",
    "        return image, label_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d10387",
   "metadata": {
    "id": "a3d10387"
   },
   "source": [
    "### YOLO Model ì •ì˜\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4831a0",
   "metadata": {
    "id": "5b4831a0"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of Yolo (v1) architecture\n",
    "with slight modification with added BatchNorm.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# yolo architecture config\n",
    "# (kerenl_size, filters, stride, padding)\n",
    "# Mì€ 2x2 maxpooling\n",
    "architecture_config = [\n",
    "    (7, 64, 2, 3),\n",
    "    \"M\",\n",
    "    (3, 192, 1, 1),\n",
    "    \"M\",\n",
    "    (1, 128, 1, 0),\n",
    "    (3, 256, 1, 1),\n",
    "    (1, 256, 1, 0),\n",
    "    (3, 512, 1, 1),\n",
    "    \"M\",\n",
    "    [(1, 256, 1, 0), (3, 512, 1, 1), 4],\n",
    "    (1, 512, 1, 0),\n",
    "    (3, 1024, 1, 1),\n",
    "    \"M\",\n",
    "    [(1, 512, 1, 0), (3, 1024, 1, 1), 2],\n",
    "    (3, 1024, 1, 1),\n",
    "    (3, 1024, 2, 1),\n",
    "    (3, 1024, 1, 1),\n",
    "    (3, 1024, 1, 1),\n",
    "]\n",
    "\n",
    "# convolution layer, batchnormalization, leakyreluë¡œ êµ¬ì„±ëœ í•˜ë‚˜ì˜ Block\n",
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        self.leakyrelu = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.leakyrelu(self.batchnorm(self.conv(x)))\n",
    "\n",
    "\n",
    "# ì‹¤ì œ ì‚¬ìš©ë  model\n",
    "class Yolov1(nn.Module):\n",
    "    def __init__(self, in_channels=3, **kwargs):\n",
    "        super(Yolov1, self).__init__()\n",
    "        self.architecture = architecture_config\n",
    "        self.in_channels = in_channels\n",
    "        self.darknet = self._create_conv_layers(self.architecture)\n",
    "        self.fcs = self._create_fcs(**kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.darknet(x)\n",
    "        return self.fcs(torch.flatten(x, start_dim=1))\n",
    "\n",
    "    # architecture configì— ë”°ë¼ì„œ darknet(backbone) ìƒì„±\n",
    "    def _create_conv_layers(self, architecture):\n",
    "        layers = []\n",
    "        in_channels = self.in_channels\n",
    "\n",
    "        for x in architecture:\n",
    "            # convolution block ìƒì„±\n",
    "            if type(x) == tuple:\n",
    "                layers += [\n",
    "                    CNNBlock(\n",
    "                        in_channels,\n",
    "                        x[1],\n",
    "                        kernel_size=x[0],\n",
    "                        stride=x[2],\n",
    "                        padding=x[3],\n",
    "                    )\n",
    "                ]\n",
    "                in_channels = x[1]\n",
    "\n",
    "            # max pooling\n",
    "            elif type(x) == str:\n",
    "                layers += [nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))]\n",
    "\n",
    "            # deeper convolution block\n",
    "            elif type(x) == list:\n",
    "                conv1 = x[0]\n",
    "                conv2 = x[1]\n",
    "                num_repeats = x[2]\n",
    "\n",
    "                for _ in range(num_repeats):\n",
    "                    layers += [\n",
    "                        CNNBlock(\n",
    "                            in_channels,\n",
    "                            conv1[1],\n",
    "                            kernel_size=conv1[0],\n",
    "                            stride=conv1[2],\n",
    "                            padding=conv1[3],\n",
    "                        )\n",
    "                    ]\n",
    "                    layers += [\n",
    "                        CNNBlock(\n",
    "                            conv1[1],\n",
    "                            conv2[1],\n",
    "                            kernel_size=conv2[0],\n",
    "                            stride=conv2[2],\n",
    "                            padding=conv2[3],\n",
    "                        )\n",
    "                    ]\n",
    "                    in_channels = conv2[1]\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    # ê·¸ë¦¬ë“œ í¬ê¸°, ë°•ìŠ¤ ê°œìˆ˜, í´ë˜ìŠ¤ ê°œìˆ˜ì— ë”°ë¼ì„œ Prediction head ìƒì„±\n",
    "    def _create_fcs(self, split_size, num_boxes, num_classes):\n",
    "        S, B, C = split_size, num_boxes, num_classes\n",
    "\n",
    "        # In original paper this should be\n",
    "        # nn.Linear(1024*S*S, 4096),\n",
    "        # nn.LeakyReLU(0.1),\n",
    "        # nn.Linear(4096, S*S*(B*5+C))\n",
    "\n",
    "        return nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024 * S * S, 496),\n",
    "            nn.Dropout(0.0),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(496, S * S * (C + B * 5)),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc81e900",
   "metadata": {
    "id": "fc81e900"
   },
   "source": [
    "### Loss function\n",
    "ğŸ‘‰ mission. Loss Function\\\n",
    "'ì½”ë“œ ì¶”ê°€' ì— ì½”ë“œë¥¼ ì¶”ê°€í•´ ì£¼ì‹œë©´ ë©ë‹ˆë‹¤! (ì–´ë ¤ìš´ ê²½ìš° ê°•ì˜ì™€ ì •ë‹µì½”ë“œë¥¼ ì°¸ê³ í•´ì„œ ì¶”ê°€í•´ì£¼ì„¸ìš”~)\n",
    "1. ì˜ˆì¸¡í•œ 2ê°œ bounding boxë“¤ê³¼ targetì˜ IoU ê³„ì‚° í•´ iou_maxes, bestbox ì„ ì •\n",
    "2. Localization Loss\n",
    "3. Confidence Loss\n",
    "4. Classification Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6813cea3",
   "metadata": {
    "id": "6813cea3"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of Yolo Loss Function from the original yolo paper\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# from utils import intersection_over_union\n",
    "\n",
    "\n",
    "class YoloLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Calculate the loss for yolo (v1) model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, S=7, B=2, C=10):\n",
    "        super(YoloLoss, self).__init__()\n",
    "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "        \"\"\"\n",
    "        S is split size of image (in paper 7),\n",
    "        B is number of boxes (in paper 2),\n",
    "        C is number of classes (Custom dataset is 10),\n",
    "        \"\"\"\n",
    "        self.S = S  # ê·¸ë¦¬ë“œ í¬ê¸°\n",
    "        self.B = B  # bounding box ìˆ˜\n",
    "        self.C = C  # class ìˆ˜\n",
    "        self.lambda_noobj = 0.5\n",
    "        self.lambda_coord = 5\n",
    "\n",
    "    def forward(self, predictions, target):\n",
    "        predictions = predictions.reshape(\n",
    "            -1, self.S, self.S, self.C + self.B * 5\n",
    "        )  # 7x7x20 feature map flatten\n",
    "\n",
    "        # ì˜ˆì¸¡í•œ 2ê°œì˜ bounding boxì˜ IoU ê³„ì‚°\n",
    "        # ì²«ë²ˆì§¸ bounding boxì™€ targetê³¼ iou ê³„ì‚°\n",
    "        iou_b1 = \"ì½”ë“œ ì¶”ê°€\"  # [..., 11:15] ì²«ë²ˆì§¸ bounding box\n",
    "        # ë‘ë²ˆì§¸ bounding boxì™€ targetê³¼ iou ê³„ì‚°\n",
    "        iou_b2 = \"ì½”ë“œ ì¶”ê°€\"  # [..., 16:20] ë‘ë²ˆì§¸ bounding box\n",
    "        ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n",
    "\n",
    "        iou_maxes, bestbox = \"ì½”ë“œ ì¶”ê°€\"  # bounding box ë‘ê°œ ì¤‘ ë” í° IoUë¥¼ ê°€ì§„ ë°•ìŠ¤\n",
    "        exists_box = target[..., 10].unsqueeze(\n",
    "            3\n",
    "        )  # í•´ë‹¹ grid cellì— ground-truthê°€ ì¡´ì¬í•˜ëŠ”ì§€ ì—¬ë¶€ (1 : ì¡´ì¬, 0 : ì¡´ì¬x)\n",
    "\n",
    "        # ======================== #\n",
    "        #     Localization Loss    #\n",
    "        # ======================== #\n",
    "\n",
    "        # box_predictions : IoU ë” í° ê°’ì˜ bounding box\n",
    "        box_predictions = exists_box * (  # ground-truthê°€ ì¡´ì¬í•˜ë©´ ì˜ˆì¸¡\n",
    "            (\"ì½”ë“œ ì¶”ê°€\" + \"ì½”ë“œ ì¶”ê°€\")  # IoUê°€ ë” í° ë°•ìŠ¤ê°€ ë‘ë²ˆì§¸ ë°•ìŠ¤ì¼ ë•Œ  # IoUê°€ ë” í° ë°•ìŠ¤ê°€ ì²«ë²ˆì§¸ ë°•ìŠ¤ì¼ ë•Œ\n",
    "        )\n",
    "\n",
    "        box_targets = exists_box * target[..., 11:15]\n",
    "\n",
    "        # width, height ë£¨íŠ¸ ì”Œìš°ê¸°\n",
    "        box_predictions[..., 2:4] = torch.sign(\"ì½”ë“œ ì¶”ê°€\") * torch.sqrt(\n",
    "            torch.abs(\"ì½”ë“œ ì¶”ê°€\" + 1e-6)\n",
    "        )\n",
    "        box_targets[..., 2:4] = torch.sqrt(\"ì½”ë“œ ì¶”ê°€\")\n",
    "\n",
    "        # MSE loss\n",
    "        box_loss = \"ì½”ë“œ ì¶”ê°€\"\n",
    "\n",
    "        # ======================== #\n",
    "        #      Confidence Loss     #\n",
    "        # ======================== #\n",
    "\n",
    "        # confidence lossëŠ” objectê°€ ìˆì„ ë•Œ, ì—†ì„ ë•Œ ë‚˜ëˆ ì„œ ê³„ì‚° (exists_box: object ì¡´ì¬ ìœ ë¬´)\n",
    "\n",
    "        ### For Object Loss ###\n",
    "\n",
    "        # pred_box : IoUê°€ í° boxì˜ confidence score\n",
    "        pred_box = \"ì½”ë“œ ì¶”ê°€\" + \"ì½”ë“œ ì¶”ê°€\"\n",
    "        # MSE Loss\n",
    "        object_loss = \"ì½”ë“œ ì¶”ê°€\"\n",
    "\n",
    "        ### For No Object Loss ###\n",
    "        # objectê°€ ì—†ì„ ë•ŒëŠ” ë‘ê°œì˜ bounding box ëª¨ë‘ ê³„ì‚°\n",
    "\n",
    "        # ì²«ë²ˆì§¸ bounding boxì˜ MSE loss\n",
    "        no_object_loss = \"ì½”ë“œ ì¶”ê°€\"\n",
    "        # ë‘ë²ˆì§¸ bounding boxì˜ MSE loss\n",
    "        no_object_loss += \"ì½”ë“œ ì¶”ê°€\"\n",
    "\n",
    "        # ======================== #\n",
    "        #    Classification Loss   #\n",
    "        # ======================== #\n",
    "\n",
    "        # MSE loss\n",
    "        class_loss = self.mse(\n",
    "            torch.flatten(\n",
    "                exists_box * predictions[..., :10],\n",
    "                end_dim=-2,\n",
    "            ),\n",
    "            torch.flatten(\n",
    "                exists_box * target[..., :10],\n",
    "                end_dim=-2,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # ======================== #\n",
    "        #         Final Loss       #\n",
    "        # ======================== #\n",
    "\n",
    "        loss = (\n",
    "            self.lambda_coord * box_loss  # localization loss\n",
    "            + object_loss  # confidence loss (object ìˆì„ ë•Œ)\n",
    "            + self.lambda_noobj * no_object_loss  # confidence loss (object ì—†ì„ ë•Œ)\n",
    "            + class_loss  # classification loss\n",
    "        )\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba67b521",
   "metadata": {
    "id": "ba67b521"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6035c5b",
   "metadata": {
    "id": "d6035c5b",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main file for training Yolo model on Custom dataset\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms.functional as FT\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "seed = 123\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Hyperparameters etc.\n",
    "LEARNING_RATE = 2e-5\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "BATCH_SIZE = 16\n",
    "WEIGHT_DECAY = 0\n",
    "EPOCHS = 100\n",
    "NUM_WORKERS = 2\n",
    "PIN_MEMORY = False\n",
    "LOAD_MODEL = False\n",
    "LOAD_MODEL_FILE = \"./yolo.pth\"\n",
    "\n",
    "\n",
    "# ì‚¬ìš©í•  transform ì •ì˜\n",
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img, bboxes):\n",
    "        for t in self.transforms:\n",
    "            img, bboxes = t(img), bboxes\n",
    "\n",
    "        return img, bboxes\n",
    "\n",
    "\n",
    "# transform init\n",
    "transform = Compose(\n",
    "    [\n",
    "        transforms.Resize((448, 448)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# train function\n",
    "def train_fn(train_loader, model, optimizer, scheduler, loss_fn):\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    mean_loss = []\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(loop):\n",
    "        # x: image, y: box ì •ë³´\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "        # yolo model output\n",
    "        out = model(x)\n",
    "\n",
    "        # outputê³¼ boxì •ë³´ (ground truth) ë¥¼ ì´ìš©í•˜ì—¬ loss ê³„ì‚°\n",
    "        loss = loss_fn(out, y)\n",
    "        mean_loss.append(loss.item())\n",
    "\n",
    "        # loss backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # update progress bar\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    return sum(mean_loss) / len(mean_loss)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # model ìƒì„±\n",
    "    model = Yolov1(split_size=7, num_boxes=2, num_classes=10).to(DEVICE)\n",
    "\n",
    "    # Adam optimizer ì´ìš©\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "    # scheduler\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(\n",
    "        optimizer, milestones=[40, 70], gamma=0.1\n",
    "    )\n",
    "\n",
    "    # Yolo loss function\n",
    "    loss_fn = YoloLoss()\n",
    "\n",
    "    # pretrained weight ì‚¬ìš©í•  ì‹œì— ë™ì‘\n",
    "    if LOAD_MODEL:\n",
    "        model.load_state_dict(torch.load(LOAD_MODEL_FILE))\n",
    "\n",
    "    # train dataset ìƒì„±\n",
    "    train_dataset = CustomDataset(\n",
    "        \"../../dataset/train.json\",\n",
    "        \"../../dataset\",\n",
    "        transforms=transform,\n",
    "    )\n",
    "\n",
    "    # test dataset ìƒì„±\n",
    "    test_dataset = CustomDataset(\n",
    "        \"../../dataset/train.json\",\n",
    "        \"../../dataset\",\n",
    "        transforms=transform,\n",
    "    )\n",
    "    # train dataset loader\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    # test dataset loader\n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        shuffle=False,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    low_loss = 1000\n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "        # model train\n",
    "        loss = train_fn(train_loader, model, optimizer, scheduler, loss_fn)\n",
    "        print(f\"#{epoch+1}: Mean loss was {loss}\")\n",
    "\n",
    "        # checkpoint ì €ì¥\n",
    "        if loss < low_loss:\n",
    "            torch.save(model.state_dict(), \"./yolo_s.pth\")\n",
    "            low_loss = loss\n",
    "\n",
    "        # í•™ìŠµëœ modelë¡œ test dataset(== train_dataset)ì˜ prediction boxì™€ target box ìƒì„±\n",
    "        pred_boxes, target_boxes = get_bboxes(\n",
    "            test_loader, model, iou_threshold=0.5, threshold=0.4\n",
    "        )\n",
    "\n",
    "        # modelì´ ì–¼ë§ˆë‚˜ ì •í™•íˆ ì˜ˆì¸¡í•˜ì˜€ëŠ”ì§€ mAPê³„ì‚°\n",
    "        mean_avg_prec = mean_average_precision(\n",
    "            pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\"\n",
    "        )\n",
    "\n",
    "        # trainì˜ mAP ê³„ì‚°\n",
    "        print(f\"Train mAP: {mean_avg_prec}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de4353f",
   "metadata": {
    "id": "0de4353f"
   },
   "source": [
    "### Reference\n",
    "https://github.com/aladdinpersson/Machine-Learning-Collection/tree/master/ML/Pytorch/object_detection/YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d979aa",
   "metadata": {},
   "source": [
    "###**ì½˜í…ì¸  ë¼ì´ì„ ìŠ¤**\n",
    "\n",
    "<font color='red'><b>**WARNING**</b></font> : **ë³¸ êµìœ¡ ì½˜í…ì¸ ì˜ ì§€ì‹ì¬ì‚°ê¶Œì€ ì¬ë‹¨ë²•ì¸ ë„¤ì´ë²„ì»¤ë„¥íŠ¸ì— ê·€ì†ë©ë‹ˆë‹¤. ë³¸ ì½˜í…ì¸ ë¥¼ ì–´ë– í•œ ê²½ë¡œë¡œë“  ì™¸ë¶€ë¡œ ìœ ì¶œ ë° ìˆ˜ì •í•˜ëŠ” í–‰ìœ„ë¥¼ ì—„ê²©íˆ ê¸ˆí•©ë‹ˆë‹¤.** ë‹¤ë§Œ, ë¹„ì˜ë¦¬ì  êµìœ¡ ë° ì—°êµ¬í™œë™ì— í•œì •ë˜ì–´ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë‚˜ ì¬ë‹¨ì˜ í—ˆë½ì„ ë°›ì•„ì•¼ í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„ë°˜í•˜ëŠ” ê²½ìš°, ê´€ë ¨ ë²•ë¥ ì— ë”°ë¼ ì±…ì„ì„ ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "(á„†á…µá„‰á…§á†«-5-á„€á…µá„‡á…©á†«) YOLO (á„†á…®á†«á„Œá…¦).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

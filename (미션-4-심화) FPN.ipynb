{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "import torch\n",
    "\n",
    "# faster rcnn model이 포함된 library\n",
    "import torchvision\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torchvision.models as models\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "from torchvision.models.detection.rpn import RegionProposalNetwork, RPNHead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    data_dir: data가 존재하는 폴더 경로\n",
    "    transforms: data transform (resize, crop, Totensor, etc,,,)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, annotation, data_dir, transforms=None):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        # coco annotation 불러오기 (coco API)\n",
    "        self.coco = COCO(annotation)\n",
    "        self.predictions = {\n",
    "            \"images\": self.coco.dataset[\"images\"].copy(),\n",
    "            \"categories\": self.coco.dataset[\"categories\"].copy(),\n",
    "            \"annotations\": None,\n",
    "        }\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "\n",
    "        image_id = self.coco.getImgIds(imgIds=index)\n",
    "\n",
    "        image_info = self.coco.loadImgs(image_id)[0]\n",
    "\n",
    "        image = cv2.imread(os.path.join(self.data_dir, image_info[\"file_name\"]))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=image_info[\"id\"])\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "        boxes = np.array([x[\"bbox\"] for x in anns])\n",
    "\n",
    "        # boxex (x_min, y_min, x_max, y_max)\n",
    "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
    "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
    "\n",
    "        # torchvision faster_rcnn은 label=0을 background로 취급\n",
    "        # class_id를 1~10으로 수정\n",
    "        labels = np.array([x[\"category_id\"] + 1 for x in anns])\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        areas = np.array([x[\"area\"] for x in anns])\n",
    "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "\n",
    "        is_crowds = np.array([x[\"iscrowd\"] for x in anns])\n",
    "        is_crowds = torch.as_tensor(is_crowds, dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": torch.tensor([index]),\n",
    "            \"area\": areas,\n",
    "            \"iscrowd\": is_crowds,\n",
    "        }\n",
    "\n",
    "        # transform\n",
    "        if self.transforms:\n",
    "            sample = {\"image\": image, \"bboxes\": target[\"boxes\"], \"labels\": labels}\n",
    "            sample = self.transforms(**sample)\n",
    "            image = sample[\"image\"]\n",
    "            target[\"boxes\"] = torch.tensor(sample[\"bboxes\"], dtype=torch.float32)\n",
    "\n",
    "        return image, target, image_id\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.coco.getImgIds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transform():\n",
    "    return A.Compose(\n",
    "        [A.Resize(1024, 1024), A.Flip(p=0.5), ToTensorV2(p=1.0)],\n",
    "        bbox_params={\"format\": \"pascal_voc\", \"label_fields\": [\"labels\"]},\n",
    "    )\n",
    "\n",
    "\n",
    "def get_valid_transform():\n",
    "    return A.Compose(\n",
    "        [ToTensorV2(p=1.0)],\n",
    "        bbox_params={\"format\": \"pascal_voc\", \"label_fields\": [\"labels\"]},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Util Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Averager:\n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(num_epochs, train_data_loader, optimizer, model, device):\n",
    "    best_loss = 1000\n",
    "    loss_hist = Averager()\n",
    "    for epoch in range(num_epochs):\n",
    "        loss_hist.reset()\n",
    "\n",
    "        for images, targets, image_ids in tqdm(train_data_loader):\n",
    "\n",
    "            # gpu 계산을 위해 image.to(device)\n",
    "            images = list(image.float().to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            # calculate loss\n",
    "            loss_dict = model(images, targets)\n",
    "\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            loss_value = losses.item()\n",
    "\n",
    "            loss_hist.send(loss_value)\n",
    "\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch #{epoch+1} loss: {loss_hist.value}\")\n",
    "        if loss_hist.value < best_loss:\n",
    "            save_path = \"./checkpoints/faster_rcnn_torchvision_checkpoints.pth\"\n",
    "            save_dir = os.path.dirname(save_path)\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.makedirs(save_dir)\n",
    "\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            best_loss = loss_hist.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FPN(nn.Module):\n",
    "    r\"\"\"Feature Pyramid Network.\n",
    "\n",
    "    This is an implementation of paper `Feature Pyramid Networks for Object\n",
    "    Detection <https://arxiv.org/abs/1612.03144>`_.\n",
    "\n",
    "    Args:\n",
    "        in_channels (List[int]): input feature map들의 channels.\n",
    "        out_channels (int): output channel\n",
    "        extra_level (bool): Number of output scales.\n",
    "            Default: `True\n",
    "        upsample_cfg (dict): Config dict for interpolate layer.\n",
    "            Default: `dict(mode='nearest')`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        extra_level=True,\n",
    "        upsample_cfg=dict(mode=\"nearest\"),\n",
    "    ):\n",
    "        super(FPN, self).__init__()\n",
    "        assert isinstance(in_channels, list)\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_ins = len(in_channels)\n",
    "        self.upsample_cfg = upsample_cfg.copy()\n",
    "        self.backbone_end_level = self.num_ins\n",
    "        self.extra_level = extra_level\n",
    "\n",
    "        self.lateral_convs = nn.ModuleList()\n",
    "        self.fpn_convs = nn.ModuleList()\n",
    "\n",
    "        \"\"\"\n",
    "        input list의 길이만큼 fpn_conv, lateral_conv 생성\n",
    "        fpn_conv: top-down 수행 전 channel을 맞춰주는 convolution\n",
    "        lateral_conv: top-down 수행 후 학습을 위해 통과하는 convolution\n",
    "        \"\"\"\n",
    "        for i in range(self.backbone_end_level):\n",
    "            l_conv = nn.Conv2d(\n",
    "                in_channels[i], out_channels, kernel_size=1, stride=1, padding=0\n",
    "            )\n",
    "            fpn_conv = nn.Conv2d(\n",
    "                out_channels, out_channels, kernel_size=3, stride=1, padding=1\n",
    "            )\n",
    "\n",
    "            self.lateral_convs.append(l_conv)\n",
    "            self.fpn_convs.append(fpn_conv)\n",
    "\n",
    "        self.normal_init(self.fpn_convs, 0, 0.01)\n",
    "        self.normal_init(self.lateral_convs, 0, 0.01)\n",
    "\n",
    "        if self.extra_level:\n",
    "            in_channels = self.in_channels[self.backbone_end_level - 1]\n",
    "            self.extra_conv = nn.Conv2d(\n",
    "                out_channels, out_channels, kernel_size=3, stride=2, padding=1\n",
    "            )\n",
    "\n",
    "            self.normal_init(self.extra_conv, 0, 0.01)\n",
    "\n",
    "    # default init_weights for conv(msra) and norm in ConvModule\n",
    "    def normal_init(self, convs, mean, stddev, truncated=False):\n",
    "        \"\"\"\n",
    "        weight initialization\n",
    "        \"\"\"\n",
    "        if isinstance(convs, nn.ModuleList):\n",
    "            for conv in convs:\n",
    "                conv.weight.data.normal_(mean, stddev)\n",
    "                conv.bias.data.zero_()\n",
    "        else:\n",
    "            convs.weight.data.normal_(mean, stddev)\n",
    "            convs.bias.data.zero_()\n",
    "\n",
    "    \"\"\"\n",
    "        inputs: list of feature maps from backbone\n",
    "        outs: list of feature maps\n",
    "                FPN을 통과한 feature map, input 과 shape 동일\n",
    "                self.extra_level인 True인 경우 feature map 하나 추가\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward function.\"\"\"\n",
    "        assert len(inputs) == len(self.in_channels)\n",
    "\n",
    "        # build laterals\n",
    "        # use self.lateral_convs\n",
    "        laterals = [\n",
    "            lateral_conv(inputs[i]) for i, lateral_conv in enumerate(self.lateral_convs)\n",
    "        ]\n",
    "\n",
    "        # build top-down path\n",
    "        # use F.interpolate(laterals[i], size, **self.upsample_cfg)\n",
    "\n",
    "        used_backbone_levels = len(laterals)\n",
    "        for i in range(used_backbone_levels - 1, 0, -1):\n",
    "            prev_shape = laterals[i - 1].shape[2:]\n",
    "            laterals[i - 1] += F.interpolate(\n",
    "                laterals[i], size=prev_shape, **self.upsample_cfg\n",
    "            )\n",
    "\n",
    "        # build outputs\n",
    "        # use self.fpn_convs\n",
    "        # part 1: from original levels\n",
    "        outs = [self.fpn_convs[i](laterals[i]) for i in range(used_backbone_levels)]\n",
    "\n",
    "        # part 2: add extra levels\n",
    "        # use self.extra_level\n",
    "        if self.extra_level:\n",
    "            outs.append(self.extra_conv(laterals[-1]))\n",
    "\n",
    "        return tuple(outs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### custom backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNextFPN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNextFPN, self).__init__()\n",
    "        self.backbone = resnext50_32x4d = models.resnext50_32x4d(pretrained=True)\n",
    "        self.fpn = FPN(\n",
    "            in_channels=[256, 512, 1024, 2048], out_channels=256, extra_level=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone.conv1(x)\n",
    "        x = self.backbone.bn1(x)\n",
    "        x = self.backbone.relu(x)\n",
    "        x = self.backbone.maxpool(x)\n",
    "\n",
    "        # 각 stage를 지나는 outs에 저장\n",
    "        outs = []\n",
    "        outs.append(self.backbone.layer1(x))\n",
    "        outs.append(self.backbone.layer2(outs[-1]))\n",
    "        outs.append(self.backbone.layer3(outs[-1]))\n",
    "        outs.append(self.backbone.layer4(outs[-1]))\n",
    "\n",
    "        # fpn 통과\n",
    "        fpn_outs = self.fpn(outs)\n",
    "\n",
    "        # 기존 torchvision backbone에서 사용하던 양식 맞추기\n",
    "        feat_list = [\"0\", \"1\", \"2\", \"3\", \"4\"]\n",
    "        out = OrderedDict([(k, v) for k, v in zip(feat_list, fpn_outs)])\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 데이터셋 불러오기\n",
    "    annotation = \"../../dataset/train.json\"  # annotation 경로\n",
    "    data_dir = \"../../dataset\"  # data_dir 경로\n",
    "    train_dataset = CustomDataset(annotation, data_dir, get_train_transform())\n",
    "    train_data_loader = DataLoader(\n",
    "        train_dataset, batch_size=4, shuffle=False, num_workers=0, collate_fn=collate_fn\n",
    "    )\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    print(device)\n",
    "\n",
    "    # torchvision model 불러오기\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    # backbone 교체\n",
    "    model.backbone = ResNextFPN()\n",
    "    num_classes = 11  # class 개수= 10 + background\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    model.to(device)\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "    optimizer = torch.optim.Adam(params, lr=0.0005, weight_decay=0.0005)\n",
    "    num_epochs = 1\n",
    "\n",
    "    # training\n",
    "    train_fn(num_epochs, train_data_loader, optimizer, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

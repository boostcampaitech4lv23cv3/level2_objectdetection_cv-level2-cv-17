{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "xsgTBr3BqE8z",
   "metadata": {
    "id": "xsgTBr3BqE8z"
   },
   "source": [
    "# Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34451fa1",
   "metadata": {
    "id": "34451fa1"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa73d33",
   "metadata": {
    "id": "2aa73d33"
   },
   "outputs": [],
   "source": [
    "# !pip install visdom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c142ed1a",
   "metadata": {
    "id": "c142ed1a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import six\n",
    "from collections import namedtuple\n",
    "\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchvision.models import vgg16\n",
    "from torchvision.ops import RoIPool\n",
    "from torchvision.ops import nms\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils import data as data_\n",
    "\n",
    "from torchnet.meter import ConfusionMeter, AverageValueMeter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2wb--qP79Xx2",
   "metadata": {
    "id": "2wb--qP79Xx2"
   },
   "source": [
    "# Util Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f218805b",
   "metadata": {
    "id": "f218805b"
   },
   "outputs": [],
   "source": [
    "def loc2bbox(src_bbox, loc):\n",
    "    \"\"\"\n",
    "    Decodes bouding boxes from bounding box offsets and scales.\n",
    "\n",
    "    Args:\n",
    "        src_bbox: A coordinates of bounding boxes.\n",
    "            These coordinates are (p_ymin, p_xmin, p_ymax, p_xmax).\n",
    "        loc: An array with offsets and scales.\n",
    "            The shapes of 'src_bbox' and 'loc' should be same.\n",
    "            This contains values: (t_y, t_x, t_h, t_w).\n",
    "    Returns: Decoded bounding box coordinates.\n",
    "    \"\"\"\n",
    "\n",
    "    if src_bbox.shape[0] == 0:\n",
    "        return np.zeros((0, 4), dtype=loc.dtype)\n",
    "\n",
    "    src_bbox = src_bbox.astype(src_bbox.dtype, copy=False)\n",
    "    src_height = src_bbox[:, 2] - src_bbox[:, 0]\n",
    "    src_width = src_bbox[:, 3] - src_bbox[:, 1]\n",
    "    src_ctr_y = src_bbox[:, 0] + 0.5 * src_height\n",
    "    src_ctr_x = src_bbox[:, 1] + 0.5 * src_width\n",
    "\n",
    "    dy = loc[:, 0::4]\n",
    "    dx = loc[:, 1::4]\n",
    "    dh = loc[:, 2::4]\n",
    "    dw = loc[:, 3::4]\n",
    "\n",
    "    ctr_y = dy * src_height[:, np.newaxis] + src_ctr_y[:, np.newaxis]\n",
    "    ctr_x = dx * src_width[:, np.newaxis] + src_ctr_x[:, np.newaxis]\n",
    "    h = np.exp(dh) * src_height[:, np.newaxis]\n",
    "    w = np.exp(dw) * src_width[:, np.newaxis]\n",
    "\n",
    "    dst_bbox = np.zeros(loc.shape, dtype=loc.dtype)\n",
    "    dst_bbox[:, 0::4] = ctr_y - 0.5 * h\n",
    "    dst_bbox[:, 1::4] = ctr_x - 0.5 * w\n",
    "    dst_bbox[:, 2::4] = ctr_y + 0.5 * h\n",
    "    dst_bbox[:, 3::4] = ctr_x + 0.5 * w\n",
    "\n",
    "    return dst_bbox\n",
    "\n",
    "\n",
    "def bbox2loc(src_bbox, dst_bbox):\n",
    "    \"\"\"\n",
    "    Encodes the source and the destination bouding boxes to \"loc\".\n",
    "\n",
    "    The offsets and scales t_y, t_x, t_h, t_w can be computed by the following formulas\n",
    "    t_y = (g_y - p_y) / p_h\n",
    "    t_x = (g_x - p_x) / p_w\n",
    "    t_h = log(g_h / p_h)\n",
    "    t_w = log(g_w / p_W)\n",
    "\n",
    "    Args:\n",
    "        src_bbox: These coordinates are (p_ymin, p_xmin, p_ymax, p_xmax).\n",
    "        dst_bbox: These coordinates are (g_ymin, g_xmin, g_ymax, g_xmax).\n",
    "\n",
    "    Returns:\n",
    "        Bounding box offsets and scales from src_bbox to dst_bbox.\n",
    "        The second axis contains four values (t_y, t_x, t_h, t_w).\n",
    "    \"\"\"\n",
    "\n",
    "    # x_min, y_min, x_max, y_max\n",
    "    height = src_bbox[:, 2] - src_bbox[:, 0]\n",
    "    width = src_bbox[:, 3] - src_bbox[:, 1]\n",
    "    ctr_y = src_bbox[:, 0] + 0.5 * height\n",
    "    ctr_x = src_bbox[:, 1] + 0.5 * width\n",
    "\n",
    "    # x_min, y_min, x_max, y_max\n",
    "    base_height = dst_bbox[:, 2] - dst_bbox[:, 0]\n",
    "    base_width = dst_bbox[:, 3] - dst_bbox[:, 1]\n",
    "    base_ctr_y = dst_bbox[:, 0] + 0.5 * base_height\n",
    "    base_ctr_x = dst_bbox[:, 1] + 0.5 * base_width\n",
    "\n",
    "    eps = np.finfo(height.dtype).eps\n",
    "    height = np.maximum(height, eps)\n",
    "    width = np.maximum(width, eps)\n",
    "\n",
    "    dy = (base_ctr_y - ctr_y) / height\n",
    "    dx = (base_ctr_x - ctr_x) / width\n",
    "    dh = np.log(base_height / height)\n",
    "    dw = np.log(base_width / width)\n",
    "\n",
    "    loc = np.vstack((dy, dx, dh, dw)).transpose()\n",
    "    return loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1DTpLq7_9Qr2",
   "metadata": {
    "id": "1DTpLq7_9Qr2"
   },
   "outputs": [],
   "source": [
    "def normal_init(m, mean, stddev, truncated=False):\n",
    "    \"\"\"\n",
    "    weight initialization\n",
    "    \"\"\"\n",
    "    if truncated:\n",
    "        m.weight.data.normal_().fmod_(2).mul_(stddev).add_(mean)\n",
    "    else:\n",
    "        m.weight.data.normal_(mean, stddev)\n",
    "        m.bias.data.zero_()\n",
    "\n",
    "\n",
    "def get_inside_index(anchor, H, W):\n",
    "    # Calc indicies of anchors which are located completely inside of the image\n",
    "    # whose size is speficied.\n",
    "    index_inside = np.where(\n",
    "        (anchor[:, 0] >= 0)\n",
    "        & (anchor[:, 1] >= 0)\n",
    "        & (anchor[:, 2] <= H)\n",
    "        & (anchor[:, 3] <= W)\n",
    "    )[0]\n",
    "    return index_inside\n",
    "\n",
    "\n",
    "def unmap(data, count, index, fill=0):\n",
    "    # Unmap a subset of item (data) back to the original set of items (of size count)\n",
    "    if len(data.shape) == 1:\n",
    "        ret = np.empty((count,), dtype=data.dtype)\n",
    "        ret.fill(fill)\n",
    "        ret[index] = data\n",
    "    else:\n",
    "        ret = np.empty((count,) + data.shape[1:], dtype=data.dtype)\n",
    "        ret.fill(fill)\n",
    "        ret[index, :] = data\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63351bbd",
   "metadata": {
    "id": "63351bbd"
   },
   "outputs": [],
   "source": [
    "def tonumpy(data):\n",
    "    if isinstance(data, np.ndarray):\n",
    "        return data\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        return data.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def totensor(data, cuda=True):\n",
    "    if isinstance(data, np.ndarray):\n",
    "        tensor = torch.from_numpy(data)\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        tensor = data.detach()\n",
    "    if cuda:\n",
    "        tensor = tensor.cuda()\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def scalar(data):\n",
    "    if isinstance(data, np.ndarray):\n",
    "        return data.reshape(1)[0]\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        return data.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PJhVDlP3qA7h",
   "metadata": {
    "id": "PJhVDlP3qA7h"
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ecf14d",
   "metadata": {
    "id": "e0ecf14d"
   },
   "source": [
    "### ÌïòÏù¥Ìçº ÌååÎùºÎØ∏ÌÑ∞ ÏÑ∏ÌåÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166d1663",
   "metadata": {
    "id": "166d1663"
   },
   "outputs": [],
   "source": [
    "epochs = 14\n",
    "learning_rate = 1e-3\n",
    "lr_decay = 0.1\n",
    "weight_decay = 0.0005\n",
    "# use dropout in RoIHead\n",
    "use_drop = False\n",
    "\n",
    "rpn_sigma = 3.0  # sigma for l1_smooth_loss (RPN loss)\n",
    "roi_sigma = 1.0  # sigma for l1_smooth_loss (ROI loss)\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ Í≤ΩÎ°ú\n",
    "data_dir = \"../../dataset\"\n",
    "# trainÏãú checkpoint Í≤ΩÎ°ú\n",
    "train_load_path = None\n",
    "# inferenceÏãú Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Í≤ΩÎ°ú\n",
    "inf_load_path = \"./checkpoints/faster_rcnn_scratch_checkpoints.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fb9a7d",
   "metadata": {
    "id": "49fb9a7d"
   },
   "source": [
    "### Dataset ÎßåÎì§Í∏∞"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a093db22",
   "metadata": {
    "id": "a093db22"
   },
   "source": [
    "#### 1. custom data Î∂àÎü¨ Ïò§Í∏∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e08cf31",
   "metadata": {
    "id": "6e08cf31"
   },
   "outputs": [],
   "source": [
    "# TrainDataset\n",
    "class TrainCustom(Dataset):\n",
    "    def __init__(self, annotation, data_dir, transforms=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            annotation: annotation ÌååÏùº ÏúÑÏπò\n",
    "            data_dir: dataÍ∞Ä Ï°¥Ïû¨ÌïòÎäî Ìè¥Îçî Í≤ΩÎ°ú\n",
    "            transforms : transform Ïó¨Î∂Ä\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        # coco annotation Î∂àÎü¨Ïò§Í∏∞ (coco API)\n",
    "        self.coco = COCO(annotation)\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "\n",
    "        # Ïù¥ÎØ∏ÏßÄ ÏïÑÏù¥Îîî Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        image_id = self.coco.getImgIds(imgIds=index)\n",
    "\n",
    "        # Ïù¥ÎØ∏ÏßÄ Ï†ïÎ≥¥ Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        image_info = self.coco.loadImgs(image_id)[0]\n",
    "\n",
    "        # Ïù¥ÎØ∏ÏßÄ Î°úÎìú\n",
    "        image = cv2.imread(os.path.join(self.data_dir, image_info[\"file_name\"]))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "\n",
    "        # Ïñ¥ÎÖ∏ÌÖåÏù¥ÏÖò ÌååÏùº Î°úÎìú\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=image_info[\"id\"])\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "        # Î∞ïÏä§ Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        boxes = np.array([x[\"bbox\"] for x in anns])\n",
    "\n",
    "        # boxes (x_min, y_min, x_max, y_max) Íº¥Î°ú Î≥ÄÌôò\n",
    "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
    "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
    "\n",
    "        # Î†àÏù¥Î∏î Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        labels = np.array([x[\"category_id\"] for x in anns])\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        # transform Ìï®Ïàò Ï†ïÏùò\n",
    "        if self.transforms:\n",
    "            scale = 1.0  # resize scale\n",
    "            H, W, _ = image.shape\n",
    "            resize_H = int(scale * H)\n",
    "            resize_W = int(scale * W)\n",
    "            transforms = get_train_transform(resize_H, resize_W)\n",
    "        else:\n",
    "            scale = 1.0\n",
    "            transforms = no_transform()\n",
    "\n",
    "        # transform\n",
    "        sample = {\"image\": image, \"bboxes\": boxes, \"labels\": labels}\n",
    "        sample = transforms(**sample)\n",
    "        image = sample[\"image\"]\n",
    "        bboxes = torch.tensor(sample[\"bboxes\"], dtype=torch.float32)\n",
    "        boxes = torch.tensor(sample[\"bboxes\"], dtype=torch.float32)\n",
    "\n",
    "        # bboxes (x_min, y_min, x_max, y_max) -> boxes (y_min, x_min, y_max, x_max)\n",
    "        boxes[:, 0] = bboxes[:, 1]\n",
    "        boxes[:, 1] = bboxes[:, 0]\n",
    "        boxes[:, 2] = bboxes[:, 3]\n",
    "        boxes[:, 3] = bboxes[:, 2]\n",
    "\n",
    "        return image, boxes, labels, scale\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.coco.getImgIds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6790ec5",
   "metadata": {
    "id": "e6790ec5"
   },
   "outputs": [],
   "source": [
    "# Test Datset\n",
    "class TestCustom(Dataset):\n",
    "    def __init__(self, annotation, data_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            annotation: annotation ÌååÏùº ÏúÑÏπò\n",
    "            data_dir: dataÍ∞Ä Ï°¥Ïû¨ÌïòÎäî Ìè¥Îçî Í≤ΩÎ°ú\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        # coco annotation Î∂àÎü¨Ïò§Í∏∞ (coco API)\n",
    "        self.coco = COCO(annotation)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "\n",
    "        # Ïù¥ÎØ∏ÏßÄ ÏïÑÏù¥Îîî Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        image_id = self.coco.getImgIds(imgIds=index)\n",
    "\n",
    "        # Ïù¥ÎØ∏ÏßÄ Ï†ïÎ≥¥ Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        image_info = self.coco.loadImgs(image_id)[0]\n",
    "\n",
    "        # Ïù¥ÎØ∏ÏßÄ Î°úÎìú\n",
    "        image = cv2.imread(os.path.join(self.data_dir, image_info[\"file_name\"]))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "        image = torch.tensor(image, dtype=torch.float).permute(2, 0, 1)\n",
    "\n",
    "        return image, image.shape[1:]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.coco.getImgIds())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc847ee7",
   "metadata": {
    "id": "bc847ee7"
   },
   "source": [
    "#### 2. Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2af672",
   "metadata": {
    "id": "6d2af672"
   },
   "outputs": [],
   "source": [
    "# Train dataset transform\n",
    "def get_train_transform(h, w):\n",
    "    return A.Compose(\n",
    "        [A.Resize(height=h, width=w), A.Flip(p=0.5), ToTensorV2(p=1.0)],\n",
    "        bbox_params={\"format\": \"pascal_voc\", \"label_fields\": [\"labels\"]},\n",
    "    )\n",
    "\n",
    "\n",
    "# No transform\n",
    "def no_transform():\n",
    "    return A.Compose(\n",
    "        [ToTensorV2(p=1.0)],  # format for pytorch tensor\n",
    "        bbox_params={\"format\": \"pascal_voc\", \"label_fields\": [\"labels\"]},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a02619",
   "metadata": {
    "id": "e8a02619"
   },
   "source": [
    "### RPN (Region Proposal Network) Ï†ïÏùò\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43368909",
   "metadata": {
    "id": "43368909"
   },
   "source": [
    "#### 1. Anchor box ÏÉùÏÑ± (generate_anchor_base)\n",
    "\n",
    "üëâ mission1. anchor box Ï¢åÌëúÍ∞í ÏÉùÏÑ±\\\n",
    ": 'ÏΩîÎìú Ï∂îÍ∞Ä' Ïóê ÏΩîÎìúÎ•º Ï∂îÍ∞ÄÌï¥ Ï£ºÏãúÎ©¥ Îê©ÎãàÎã§! \n",
    "  (Ïñ¥Î†§Ïö∞Ïã† Í≤ΩÏö∞ÏóêÎäî Í∞ïÏùòÎ•º Ï∞∏Í≥†Ìï¥ÏÑú Ï∂îÍ∞ÄÌï¥Ï£ºÏÑ∏Ïöî~)\n",
    "1. Ï§ëÏ†ê ÎßåÎì§Í∏∞ (base_sizeÏùò Ï†àÎ∞ò)\n",
    "\n",
    "\n",
    "2. ÌïòÎÇòÏùò Ï§ëÏ†êÎãπ ratioÏôÄ anchor scalesÏóê Îî∞Îùº 9Í∞úÏùò anchor boxÏùò Ï¢åÌëúÍ∞í ÎßåÎì§Í∏∞\\\n",
    "    anchor boxÏùò Ï¢åÌëúÍ∞í : (y_min, x_min, y_max, x_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2155d0ec",
   "metadata": {
    "id": "2155d0ec"
   },
   "outputs": [],
   "source": [
    "def generate_anchor_base(base_size=16, ratios=[0.5, 1, 2], anchor_scales=[8, 16, 32]):\n",
    "    \"\"\" \n",
    "    Args:\n",
    "        ratios: ÎπÑÏú®\n",
    "        anchor_scales: Ïä§ÏºÄÏùº\n",
    "    Returns: basic anchor boxes, shape=(R, 4)\n",
    "        R: len(ratio) * len(anchor_scales) = anchor Í∞úÏàò = 9\n",
    "        4: anchor box Ï¢åÌëú Í∞í\n",
    "    \"\"\"\n",
    "    \n",
    "    py = 'ÏΩîÎìú Ï∂îÍ∞Ä' # center y\n",
    "    px = 'ÏΩîÎìú Ï∂îÍ∞Ä'. # center x\n",
    "\n",
    "    anchor_base = np.zeros((len(ratios) * len(anchor_scales), 4), dtype=np.float32) # Îπà achor box anchor_box\n",
    "    \n",
    "    for i in six.moves.range(len(ratios)):\n",
    "        for j in six.moves.range(len(anchor_scales)):\n",
    "            h = base_size * anchor_scales[j] * np.sqrt(ratios[i])       # height\n",
    "            w = base_size * anchor_scales[j] * np.sqrt(1. / ratios[i])  # width\n",
    "\n",
    "            index = i * len(anchor_scales) + j\n",
    "            \n",
    "            # offset of anchor box\n",
    "            anchor_base[index, 0] = 'ÏΩîÎìú Ï∂îÍ∞Ä' # y_min\n",
    "            anchor_base[index, 1] = 'ÏΩîÎìú Ï∂îÍ∞Ä' # x_min\n",
    "            anchor_base[index, 2] = 'ÏΩîÎìú Ï∂îÍ∞Ä' # y_max\n",
    "            anchor_base[index, 3] = 'ÏΩîÎìú Ï∂îÍ∞Ä' # x_max\n",
    "            \n",
    "    return anchor_base # (9,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74ef20d",
   "metadata": {
    "id": "e74ef20d"
   },
   "source": [
    "#### 2. Proposal ÏÉùÏÑ± (ProposalCreator)\n",
    "RPNÏóêÏÑú Íµ¨Ìïú rpn_locÏôÄ anchorÏùÑ ÌÜµÌï¥ÏÑú Region of Interest(RoI)Î•º ÏÉùÏÑ±\\\n",
    "RoI Í∞úÏàò Ï§ÑÏù¥Í∏∞ ÏúÑÌï¥ÏÑú ÎØ∏Î¶¨ Ï†ïÌï¥Îëî ÌÅ¨Í∏∞(min_size)Ïóê ÎßûÎäî roiÎì§ Ï§ë NMSÎ•º ÌÜµÌï¥ ÏµúÏ¢Ö RoI Î∞òÌôò (train Ïãú 2000Í∞ú)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af7541e",
   "metadata": {
    "id": "4af7541e"
   },
   "outputs": [],
   "source": [
    "class ProposalCreator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        parent_model,\n",
    "        nms_thresh=0.7,  # nms threshold\n",
    "        n_train_pre_nms=12000,  # trainÏãú nms Ï†Ñ roi Í∞úÏàò\n",
    "        n_train_post_nms=2000,  # trainÏãú nms ÌõÑ roi Í∞úÏàò\n",
    "        n_test_pre_nms=6000,  # testÏãú nms Ï†Ñ roi Í∞úÏàò\n",
    "        n_test_post_nms=300,  # testÏãú nms ÌõÑ roi Í∞úÏàò\n",
    "        min_size=16,\n",
    "    ):\n",
    "        self.parent_model = parent_model\n",
    "        self.nms_thresh = nms_thresh\n",
    "        self.n_train_pre_nms = n_train_pre_nms\n",
    "        self.n_train_post_nms = n_train_post_nms\n",
    "        self.n_test_pre_nms = n_test_pre_nms\n",
    "        self.n_test_post_nms = n_test_post_nms\n",
    "        self.min_size = min_size\n",
    "\n",
    "    def __call__(self, loc, score, anchor, img_size, scale=1.0):\n",
    "        if self.parent_model.training:  # trainÏ§ëÏùº Îïå\n",
    "            n_pre_nms = self.n_train_pre_nms\n",
    "            n_post_nms = self.n_train_post_nms\n",
    "        else:  # testÏ§ëÏùº Îïå\n",
    "            n_pre_nms = self.n_test_pre_nms\n",
    "            n_post_nms = self.n_test_post_nms\n",
    "\n",
    "        # anchorÏùò Ï¢åÌëúÍ∞íÍ≥º predicted bounding bounding box offset(y,x,h,w)Î•º ÌÜµÌï¥\n",
    "        # bounding box Ï¢åÌëúÍ∞í(y_min, x_min, y_max, x_max) ÏÉùÏÑ±\n",
    "        roi = loc2bbox(anchor, loc)\n",
    "\n",
    "        # Clip predicted boxes to image.\n",
    "        roi[:, slice(0, 4, 2)] = np.clip(roi[:, slice(0, 4, 2)], 0, img_size[0])\n",
    "        roi[:, slice(1, 4, 2)] = np.clip(roi[:, slice(1, 4, 2)], 0, img_size[1])\n",
    "\n",
    "        # min_size Î≥¥Îã§ ÏûëÏùÄ boxÎì§ÏùÄ Ï†úÍ±∞\n",
    "        min_size = self.min_size * scale\n",
    "        hs = roi[:, 2] - roi[:, 0]  # height\n",
    "        ws = roi[:, 3] - roi[:, 1]  # width\n",
    "        keep = np.where((hs >= min_size) & (ws >= min_size))[0]\n",
    "        roi = roi[keep, :]\n",
    "        score = score[keep]\n",
    "\n",
    "        # Sort all (proposal, score) pairs by score from highest to lowest.\n",
    "        # Take top pre_nms_topN\n",
    "        order = score.ravel().argsort()[::-1]\n",
    "        if n_pre_nms > 0:\n",
    "            order = order[:n_pre_nms]\n",
    "        roi = roi[order, :]\n",
    "        score = score[order]\n",
    "\n",
    "        # nms Ï†ÅÏö©\n",
    "        keep = nms(\n",
    "            torch.from_numpy(roi).cuda(),\n",
    "            torch.from_numpy(score).cuda(),\n",
    "            self.nms_thresh,\n",
    "        )\n",
    "        if n_post_nms > 0:\n",
    "            keep = keep[:n_post_nms]\n",
    "        roi = roi[keep.cpu().numpy()]\n",
    "\n",
    "        return roi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a991ada",
   "metadata": {
    "id": "7a991ada"
   },
   "source": [
    "#### 3. region proposal network\n",
    "\n",
    "VGG16 ÌÜµÍ≥ºÌïú feature mapÏúºÎ°úÎ∂ÄÌÑ∞ region proposalÎì§ ÏÉùÏÑ±\n",
    "\n",
    "üëâ mission2. Region Proposal Network\\\n",
    "'ÏΩîÎìú Ï∂îÍ∞Ä' Ïóê ÏΩîÎìúÎ•º Ï∂îÍ∞ÄÌï¥ Ï£ºÏãúÎ©¥ Îê©ÎãàÎã§! (Ïñ¥Î†§Ïö∞Ïã† Í≤ΩÏö∞ Í∞ïÏùòÎ•º Ï∞∏Í≥†Ìï¥ÏÑú Ï∂îÍ∞ÄÌï¥Ï£ºÏÑ∏Ïöî~)\\\n",
    " **tensor shapeÏùÄ Î™®Îëê 1024x1024 Ïù¥ÎØ∏ÏßÄ Í∏∞Ï§ÄÏûÖÎãàÎã§.**\n",
    "1. backboneÏóêÏÑú ÎÇòÏò® feature mapÏóê 3x3 conv Ïó∞ÏÇ∞ÏùÑ Ï†ÅÏö©ÌïòÏó¨ Ï§ëÍ∞Ñ feature map ÏÉùÏÑ± \\\n",
    "    input: x (torch.Size([1, 512, 64, 64]))\\\n",
    "    output: middle (torch.Size([1, 512, 64, 64]))\n",
    "    \n",
    "    \n",
    "2. middle(Ï§ëÍ∞Ñ feature map)Ïóê 1x1 conv Ïó∞ÏÇ∞ÏùÑ Ï†ÅÏö©ÌïòÏó¨ 9x4(anchor boxÏùò Ïàò x bounding box Ï¢åÌëúÍ∞í)Í∞úÏùò channelÏùÑ Í∞ÄÏßÄÎäî feature map ÏÉùÏÑ±\\\n",
    "    input: middle (torch.Size([1, 512, 64, 64]))\\\n",
    "    output: rpn_locs (torch.Size([1, 36, 64, 64]))\n",
    "    \n",
    "    \n",
    "3. middle(Ï§ëÍ∞Ñ feature map)Ïóê 1x1 conv Ïó∞ÏÇ∞ÏùÑ Ï†ÅÏö©ÌïòÏó¨ 9x2(anchor boxÏùò Ïàò x object Ïó¨Î∂Ä)Í∞úÏùò channelÏùÑ Í∞ÄÏßÄÎäî feature map ÏÉùÏÑ±\\\n",
    "    input: middle (torch.Size([1, 512, 64, 64]))\\\n",
    "    output: rpn_locs (torch.Size([1, 18, 64, 64]))\n",
    "    \n",
    "    \n",
    "4. Proposal Creator Ìï®ÏàòÎ•º ÏÇ¨Ïö©ÌïòÏó¨ roi ÏÉùÏÑ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb749c00",
   "metadata": {
    "id": "cb749c00"
   },
   "outputs": [],
   "source": [
    "class RegionProposalNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=512,\n",
    "        mid_channels=512,\n",
    "        ratios=[0.5, 1, 2],\n",
    "        anchor_scales=[8, 16, 32],\n",
    "        feat_stride=16,\n",
    "        proposal_creator_params=dict(),\n",
    "    ):\n",
    "\n",
    "        super(RegionProposalNetwork, self).__init__()\n",
    "\n",
    "        self.anchor_base = generate_anchor_base(\n",
    "            anchor_scales=anchor_scales, ratios=ratios\n",
    "        )  # 9Í∞úÏùò anchorbox Ï¢åÌëúÍ∞í ÏÉùÏÑ±\n",
    "        self.feat_stride = feat_stride\n",
    "        self.proposal_layer = ProposalCreator(\n",
    "            self, **proposal_creator_params\n",
    "        )  # proposal_creator_params : Ìï¥Îãπ ÎÑ§Ìä∏ÏõåÌÅ¨Í∞Ä trainingÏù∏ÏßÄ testingÏù∏ÏßÄ ÏïåÎ†§Ï§ÄÎã§.\n",
    "        n_anchor = self.anchor_base.shape[0]  # base anchor Í∞úÏàò = 9Í∞ú\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, mid_channels, 3, 1, 1)\n",
    "        self.loc = nn.Conv2d(mid_channels, n_anchor * 4, 1, 1, 0)  # 9*4\n",
    "        self.score = nn.Conv2d(mid_channels, n_anchor * 2, 1, 1, 0)  # 9*2\n",
    "\n",
    "        normal_init(self.conv1, 0, 0.01)  # weight initalizer\n",
    "        normal_init(self.score, 0, 0.01)  # weight initalizer\n",
    "        normal_init(self.loc, 0, 0.01)  # weight initalizer\n",
    "\n",
    "    def forward(self, x, img_size, scale=1.0):\n",
    "        # x(feature map) : torch.Size([1, 512, 64, 64])\n",
    "        n, _, hh, ww = x.shape\n",
    "\n",
    "        # Ï†ÑÏ≤¥ (h*w*9)Í∞ú anchorÏùò Ï¢åÌëúÍ∞í # anchor_base:(9, 4)\n",
    "        anchor = _enumerate_shifted_anchor(\n",
    "            np.array(self.anchor_base), self.feat_stride, hh, ww\n",
    "        )  # (36864, 4)\n",
    "        n_anchor = anchor.shape[0] // (hh * ww)  # anchor Í∞úÏàò = 9Í∞ú\n",
    "\n",
    "        # feature mapÏóê 3x3 conv Ïó∞ÏÇ∞ÏùÑ Ï†ÅÏö©ÌïòÏó¨ Ï§ëÍ∞Ñ feature map ÏÉùÏÑ±\n",
    "        middle = F.relu(\"ÏΩîÎìú Ï∂îÍ∞Ä\")  # torch.Size([1, 512, 64, 64])\n",
    "\n",
    "        # predicted bounding box offset\n",
    "        # middle(Ï§ëÍ∞Ñ feature map)Ïóê 1x1 conv Ïó∞ÏÇ∞ÏùÑ Ï†ÅÏö©ÌïòÏó¨ 9x4(anchor boxÏùò Ïàò x bounding box Ï¢åÌëúÍ∞í)Í∞úÏùò channelÏùÑ Í∞ÄÏßÄÎäî feature map ÏÉùÏÑ±\n",
    "        rpn_locs = \"ÏΩîÎìú Ï∂îÍ∞Ä\"  # torch.Size([1, 36, 64, 64])\n",
    "        rpn_locs = (\n",
    "            rpn_locs.permute(0, 2, 3, 1).contiguous().view(n, -1, 4)\n",
    "        )  # torch.Size([1, 36864, 4])\n",
    "\n",
    "        # predicted scores for anchor (foreground or background)\n",
    "        # middle(Ï§ëÍ∞Ñ feature map)Ïóê 1x1 conv Ïó∞ÏÇ∞ÏùÑ Ï†ÅÏö©ÌïòÏó¨ 9x2(anchor boxÏùò Ïàò x object Ïó¨Î∂Ä)Í∞úÏùò channelÏùÑ Í∞ÄÏßÄÎäî feature map ÏÉùÏÑ±\n",
    "        rpn_scores = \"ÏΩîÎìú Ï∂îÍ∞Ä\"  # torch.Size([1, 18, 64, 64])\n",
    "        rpn_scores = rpn_scores.permute(\n",
    "            0, 2, 3, 1\n",
    "        ).contiguous()  # torch.Size([1, 64, 64, 18])\n",
    "\n",
    "        # scores for foreground\n",
    "        rpn_softmax_scores = F.softmax(\n",
    "            rpn_scores.view(n, hh, ww, n_anchor, 2), dim=4\n",
    "        )  # torch.Size([1, 64, 64, 9, 2])\n",
    "        rpn_fg_scores = rpn_softmax_scores[\n",
    "            :, :, :, :, 1\n",
    "        ].contiguous()  # torch.Size([1, 64, 64, 9])\n",
    "        rpn_fg_scores = rpn_fg_scores.view(n, -1)  # torch.Size([1, 36864])\n",
    "\n",
    "        rpn_scores = rpn_scores.view(n, -1, 2)  # torch.Size([1, 36864, 2])\n",
    "\n",
    "        # proposalÏÉùÏÑ± (ProposalCreator)\n",
    "        rois = list()  # proposalÏùò Ï¢åÌëúÍ∞íÏù¥ ÏûàÎäî bounding box array\n",
    "        roi_indices = list()  # roiÏóê Ìï¥ÎãπÌïòÎäî image Ïù∏Îç±Ïä§\n",
    "        for i in range(n):\n",
    "            # Proposal Creator(self.proposal_layer) Ìï®ÏàòÎ•º ÏÇ¨Ïö©ÌïòÏó¨ roi ÏÉùÏÑ±\n",
    "            roi = \"ÏΩîÎìú Ï∂îÍ∞Ä\"\n",
    "            batch_index = i * np.ones((len(roi),), dtype=np.int32)\n",
    "            rois.append(roi)\n",
    "            roi_indices.append(batch_index)\n",
    "        rois = np.concatenate(rois, axis=0)\n",
    "        roi_indices = np.concatenate(roi_indices, axis=0)\n",
    "\n",
    "        return rpn_locs, rpn_scores, rois, roi_indices, anchor\n",
    "\n",
    "\n",
    "def _enumerate_shifted_anchor(anchor_base, feat_stride, height, width):\n",
    "    # anchor_baseÎäî ÌïòÎÇòÏùò pixelÏóê 9Í∞ú Ï¢ÖÎ•òÏùò anchor boxÎ•º ÎÇòÌÉÄÎÉÑ\n",
    "    # Ïù¥Í≤ÉÏùÑ enumerateÏãúÏºú Ï†ÑÏ≤¥ Ïù¥ÎØ∏ÏßÄÏùò pixelÏóê Í∞ÅÍ∞Å 9Í∞úÏùò anchor boxÎ•º Í∞ÄÏßÄÍ≤å Ìï®\n",
    "    # 64x64 feature mapÏóêÏÑúÎäî 64x64x9=36864Í∞úÏùò anchor boxÍ∞ÄÏßê\n",
    "\n",
    "    shift_y = np.arange(0, height * feat_stride, feat_stride)\n",
    "    shift_x = np.arange(0, width * feat_stride, feat_stride)\n",
    "    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n",
    "    shift = np.stack(\n",
    "        (shift_y.ravel(), shift_x.ravel(), shift_y.ravel(), shift_x.ravel()), axis=1\n",
    "    )\n",
    "\n",
    "    A = anchor_base.shape[0]\n",
    "    K = shift.shape[0]\n",
    "    anchor = anchor_base.reshape((1, A, 4)) + shift.reshape((1, K, 4)).transpose(\n",
    "        (1, 0, 2)\n",
    "    )\n",
    "    anchor = anchor.reshape((K * A, 4)).astype(np.float32)\n",
    "    return anchor  # (9216, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1ff0bd",
   "metadata": {
    "id": "4e1ff0bd"
   },
   "source": [
    "### Feature extractor(VGG) Ï†ïÏùò\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9911df3f",
   "metadata": {
    "id": "9911df3f"
   },
   "outputs": [],
   "source": [
    "def decom_vgg16():\n",
    "    # the 30th layer of features is relu of conv5_3\n",
    "    model = vgg16(pretrained=True)\n",
    "\n",
    "    features = list(model.features)[:30]\n",
    "    classifier = model.classifier\n",
    "\n",
    "    classifier = list(classifier)\n",
    "    del classifier[6]\n",
    "    if not use_drop:\n",
    "        del classifier[5]\n",
    "        del classifier[2]\n",
    "    classifier = nn.Sequential(*classifier)\n",
    "\n",
    "    # freeze top4 conv\n",
    "    for layer in features[:10]:\n",
    "        for p in layer.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    return nn.Sequential(*features), classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cb8f86",
   "metadata": {
    "id": "c1cb8f86"
   },
   "source": [
    "### Faster R-CNN head Ï†ïÏùò\n",
    "\n",
    "RoI pool ÌõÑÏóê classifier, regressor ÌÜµÍ≥º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8ceb71",
   "metadata": {
    "id": "ce8ceb71"
   },
   "outputs": [],
   "source": [
    "class VGG16RoIHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Faster R-CNN head\n",
    "    RoI pool ÌõÑÏóê classifier, regressior ÌÜµÍ≥º\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_class, roi_size, spatial_scale, classifier):\n",
    "        super(VGG16RoIHead, self).__init__()\n",
    "\n",
    "        self.classifier = classifier\n",
    "        self.cls_loc = nn.Linear(4096, n_class * 4)  # bounding box regressor\n",
    "        self.score = nn.Linear(4096, n_class)  # Classifier\n",
    "\n",
    "        normal_init(self.cls_loc, 0, 0.001)  # weight initialize\n",
    "        normal_init(self.score, 0, 0.01)  # weight initialize\n",
    "\n",
    "        self.n_class = n_class  # Î∞∞Í≤Ω Ìè¨Ìï®Ìïú class Ïàò\n",
    "        self.roi_size = roi_size  # RoI-pooling ÌõÑ feature mapÏùò  ÎÜíÏù¥, ÎÑàÎπÑ\n",
    "        self.spatial_scale = spatial_scale  # roi resize scale\n",
    "        self.roi = RoIPool((self.roi_size, self.roi_size), self.spatial_scale)\n",
    "\n",
    "    def forward(self, x, rois, roi_indices):\n",
    "        # in case roi_indices is  ndarray\n",
    "        roi_indices = totensor(roi_indices).float()\n",
    "        rois = totensor(rois).float()\n",
    "        indices_and_rois = torch.cat([roi_indices[:, None], rois], dim=1)\n",
    "        # NOTE: important: yx->xy\n",
    "        xy_indices_and_rois = indices_and_rois[:, [0, 2, 1, 4, 3]]\n",
    "        indices_and_rois = xy_indices_and_rois.contiguous()  # torch.Size([128, 5])\n",
    "\n",
    "        # Í∞Å Ïù¥ÎØ∏ÏßÄ roi pooling\n",
    "        pool = self.roi(x, indices_and_rois)  # torch.Size([128, 512, 7, 7])\n",
    "        # flatten\n",
    "        pool = pool.view(pool.size(0), -1)  # torch.Size([128, 25088])\n",
    "        # fully connected\n",
    "        fc7 = self.classifier(pool)  # torch.Size([128, 4096])\n",
    "        # regression\n",
    "        roi_cls_locs = self.cls_loc(fc7)  # torch.Size([128, 44])\n",
    "        # softmax\n",
    "        roi_scores = self.score(fc7)  # torch.Size([128, 11])\n",
    "\n",
    "        return roi_cls_locs, roi_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cec189",
   "metadata": {
    "id": "85cec189"
   },
   "source": [
    "### Faster R-CNN Ï†ïÏùò\n",
    "Feature Extraction : imageÎ°úÎ∂ÄÌÑ∞ feature map ÏÉùÏÑ±\\\n",
    "Region Proposal Networks : Region of Interest ÏÉùÏÑ±\\\n",
    "Localization and Classification Head : RoIÏóê Ìï¥ÎãπÌïòÎäî feature mapÏùÑ ÏµúÏ¢Ö detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a04aba6",
   "metadata": {
    "id": "0a04aba6"
   },
   "outputs": [],
   "source": [
    "def nograd(f):\n",
    "    def new_f(*args, **kwargs):\n",
    "        with torch.no_grad():\n",
    "            return f(*args, **kwargs)\n",
    "\n",
    "    return new_f\n",
    "\n",
    "\n",
    "class FasterRCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        extractor,\n",
    "        rpn,\n",
    "        head,\n",
    "        loc_normalize_mean=(0.0, 0.0, 0.0, 0.0),\n",
    "        loc_normalize_std=(0.1, 0.1, 0.2, 0.2),\n",
    "    ):\n",
    "        super(FasterRCNN, self).__init__()\n",
    "        self.extractor = extractor  # extractor : vgg\n",
    "        self.rpn = rpn  # rpn : region proposal network\n",
    "        self.head = head  # head : RoiHead\n",
    "\n",
    "        # mean and std\n",
    "        self.loc_normalize_mean = loc_normalize_mean\n",
    "        self.loc_normalize_std = loc_normalize_std\n",
    "        self.use_preset()\n",
    "\n",
    "    @property\n",
    "    def n_class(self):  # ÏµúÏ¢Ö class Í∞úÏàò (Î∞∞Í≤Ω Ìè¨Ìï®)\n",
    "        return self.head.n_class\n",
    "\n",
    "    # predict Ïãú ÏÇ¨Ïö©ÌïòÎäî forward\n",
    "    # train Ïãú FasterRCNNTrainerÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ FasterRcnnÏóê ÏûàÎäî extractor, rpn, headÎ•º Î™®ÎìàÎ≥ÑÎ°ú Î∂àÎü¨ÏôÄÏÑú forward\n",
    "    def forward(self, x, scale=1.0):\n",
    "        img_size = x.shape[2:]\n",
    "\n",
    "        h = self.extractor(x)  # extractor ÌÜµÍ≥º\n",
    "        rpn_locs, rpn_scores, rois, roi_indices, anchor = self.rpn(\n",
    "            h, img_size, scale\n",
    "        )  # rpn ÌÜµÍ≥º\n",
    "        roi_cls_locs, roi_scores = self.head(h, rois, roi_indices)  # head ÌÜµÍ≥º\n",
    "        return roi_cls_locs, roi_scores, rois, roi_indices\n",
    "\n",
    "    def use_preset(self):  # prediction Í≥ºÏ†ï Ïì∞Ïù¥Îäî threshold Ï†ïÏùò\n",
    "        self.nms_thresh = 0.3\n",
    "        self.score_thresh = 0.05\n",
    "\n",
    "    def _suppress(self, raw_cls_bbox, raw_prob):\n",
    "        bbox = list()\n",
    "        label = list()\n",
    "        score = list()\n",
    "\n",
    "        # skip cls_id = 0 because it is the background class\n",
    "        for l in range(1, self.n_class):\n",
    "            cls_bbox_l = raw_cls_bbox.reshape((-1, self.n_class, 4))[:, l, :]\n",
    "            prob_l = raw_prob[:, l]\n",
    "            mask = prob_l > self.score_thresh\n",
    "            cls_bbox_l = cls_bbox_l[mask]\n",
    "            prob_l = prob_l[mask]\n",
    "            keep = nms(cls_bbox_l, prob_l, self.nms_thresh)\n",
    "            bbox.append(cls_bbox_l[keep].cpu().numpy())\n",
    "            # The labels are in [0, self.n_class - 2].\n",
    "            label.append((l - 1) * np.ones((len(keep),)))\n",
    "            score.append(prob_l[keep].cpu().numpy())\n",
    "\n",
    "        bbox = np.concatenate(bbox, axis=0).astype(np.float32)\n",
    "        label = np.concatenate(label, axis=0).astype(np.int32)\n",
    "        score = np.concatenate(score, axis=0).astype(np.float32)\n",
    "        return bbox, label, score\n",
    "\n",
    "    @nograd\n",
    "    def predict(self, imgs, sizes=None):\n",
    "        \"\"\"\n",
    "        Ïù¥ÎØ∏ÏßÄÏóêÏÑú Í∞ùÏ≤¥ Í≤ÄÏ∂ú\n",
    "        Input : images\n",
    "        Output : bboxes, labels, scores\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        prepared_imgs = imgs\n",
    "\n",
    "        bboxes = list()\n",
    "        labels = list()\n",
    "        scores = list()\n",
    "        for img, size in zip(prepared_imgs, sizes):\n",
    "            img = totensor(img[None]).float()\n",
    "            scale = img.shape[3] / size[1]\n",
    "            roi_cls_loc, roi_scores, rois, _ = self(\n",
    "                img, scale=scale\n",
    "            )  # self = FasterRCNN\n",
    "            # We are assuming that batch size is 1.\n",
    "            roi_score = roi_scores.data\n",
    "            roi_cls_loc = roi_cls_loc.data\n",
    "            roi = totensor(rois) / scale\n",
    "\n",
    "            # Convert predictions to bounding boxes in image coordinates.\n",
    "            # Bounding boxes are scaled to the scale of the input images.\n",
    "            mean = (\n",
    "                torch.Tensor(self.loc_normalize_mean).cuda().repeat(self.n_class)[None]\n",
    "            )\n",
    "            std = torch.Tensor(self.loc_normalize_std).cuda().repeat(self.n_class)[None]\n",
    "\n",
    "            roi_cls_loc = roi_cls_loc * std + mean\n",
    "            roi_cls_loc = roi_cls_loc.view(-1, self.n_class, 4)\n",
    "            roi = roi.view(-1, 1, 4).expand_as(roi_cls_loc)\n",
    "            cls_bbox = loc2bbox(\n",
    "                tonumpy(roi).reshape((-1, 4)), tonumpy(roi_cls_loc).reshape((-1, 4))\n",
    "            )\n",
    "            cls_bbox = totensor(cls_bbox)\n",
    "            cls_bbox = cls_bbox.view(-1, self.n_class * 4)\n",
    "            # clip bounding box\n",
    "            cls_bbox[:, 0::2] = (cls_bbox[:, 0::2]).clamp(min=0, max=size[0])\n",
    "            cls_bbox[:, 1::2] = (cls_bbox[:, 1::2]).clamp(min=0, max=size[1])\n",
    "\n",
    "            prob = F.softmax(totensor(roi_score), dim=1)\n",
    "\n",
    "            bbox, label, score = self._suppress(cls_bbox, prob)\n",
    "            bboxes.append(bbox)\n",
    "            labels.append(label)\n",
    "            scores.append(score)\n",
    "\n",
    "        self.use_preset()\n",
    "        self.train()\n",
    "        return bboxes, labels, scores\n",
    "\n",
    "    def get_optimizer(self):\n",
    "        \"\"\"\n",
    "        Optimizer ÏÑ†Ïñ∏\n",
    "        \"\"\"\n",
    "        lr = learning_rate\n",
    "        params = []\n",
    "        for key, value in dict(self.named_parameters()).items():\n",
    "            if value.requires_grad:\n",
    "                if \"bias\" in key:\n",
    "                    params += [{\"params\": [value], \"lr\": lr * 2, \"weight_decay\": 0}]\n",
    "                else:\n",
    "                    params += [\n",
    "                        {\"params\": [value], \"lr\": lr, \"weight_decay\": weight_decay}\n",
    "                    ]\n",
    "        self.optimizer = torch.optim.SGD(params, momentum=0.9)\n",
    "        return self.optimizer\n",
    "\n",
    "    def scale_lr(self, decay=0.1):\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group[\"lr\"] *= decay\n",
    "        return self.optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39849b21",
   "metadata": {
    "id": "39849b21"
   },
   "source": [
    "### Faster R-CNN ÏÉùÏÑ±\n",
    "Extractor(VGG) + RPN + Head Ìï©ÏπòÍ∏∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32726880",
   "metadata": {
    "id": "32726880"
   },
   "outputs": [],
   "source": [
    "class FasterRCNNVGG16(FasterRCNN):\n",
    "\n",
    "    feat_stride = 16  # downsample 16x for output of conv5 in vgg16\n",
    "\n",
    "    def __init__(\n",
    "        self, n_fg_class=10, ratios=[0.5, 1, 2], anchor_scales=[8, 16, 32]\n",
    "    ):  # n_fg_class : Î∞∞Í≤ΩÌè¨Ìï® ÌïòÏßÄ ÏïäÏùÄ class Í∞úÏàò\n",
    "        extractor, classifier = decom_vgg16()\n",
    "\n",
    "        rpn = RegionProposalNetwork(\n",
    "            512,\n",
    "            512,\n",
    "            ratios=ratios,\n",
    "            anchor_scales=anchor_scales,\n",
    "            feat_stride=self.feat_stride,\n",
    "        )\n",
    "\n",
    "        head = VGG16RoIHead(\n",
    "            n_class=n_fg_class + 1,\n",
    "            roi_size=7,\n",
    "            spatial_scale=(1.0 / self.feat_stride),\n",
    "            classifier=classifier,\n",
    "        )\n",
    "        super(FasterRCNNVGG16, self).__init__(\n",
    "            extractor,\n",
    "            rpn,\n",
    "            head,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46202f13",
   "metadata": {
    "id": "46202f13"
   },
   "source": [
    "### Trainer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61acb16a",
   "metadata": {
    "id": "61acb16a"
   },
   "source": [
    "#### 0. util Ìï®Ïàò Ï†ïÏùò\n",
    "bouningb box IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d25d7e",
   "metadata": {
    "id": "c9d25d7e"
   },
   "outputs": [],
   "source": [
    "def bbox_iou(bbox_a, bbox_b):\n",
    "    if bbox_a.shape[1] != 4 or bbox_b.shape[1] != 4:\n",
    "        raise IndexError\n",
    "\n",
    "    # bbox_a 1Í∞úÏôÄ bbox_b kÍ∞úÎ•º ÎπÑÍµêÌï¥ÏïºÌïòÎØÄÎ°ú NoneÏùÑ Ïù¥Ïö©Ìï¥ÏÑú Ï∞®ÏõêÏùÑ ÎäòÎ†§ÏÑú Ïó∞ÏÇ∞ÌïúÎã§.\n",
    "    # top left\n",
    "    tl = np.maximum(bbox_a[:, None, :2], bbox_b[:, :2])\n",
    "    # bottom right\n",
    "    br = np.minimum(bbox_a[:, None, 2:], bbox_b[:, 2:])\n",
    "\n",
    "    area_i = np.prod(br - tl, axis=2) * (tl < br).all(axis=2)\n",
    "    area_a = np.prod(bbox_a[:, 2:] - bbox_a[:, :2], axis=1)\n",
    "    area_b = np.prod(bbox_b[:, 2:] - bbox_b[:, :2], axis=1)\n",
    "    return area_i / (area_a[:, None] + area_b - area_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3ce234",
   "metadata": {
    "id": "ac3ce234"
   },
   "source": [
    "#### 1. Anchor Target Creator\n",
    "Anchor boxÏóê Ìï¥ÎãπÌïòÎäî ground truth bounding box match\\\n",
    "Region Proposal Network loss Íµ¨Ìï† Îïå ground truthÎ°ú ÏÇ¨Ïö©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a1e360",
   "metadata": {
    "id": "69a1e360"
   },
   "outputs": [],
   "source": [
    "class AnchorTargetCreator(object):\n",
    "    def __init__(\n",
    "        self, n_sample=256, pos_iou_thresh=0.7, neg_iou_thresh=0.3, pos_ratio=0.5\n",
    "    ):\n",
    "        self.n_sample = n_sample\n",
    "        self.pos_iou_thresh = pos_iou_thresh\n",
    "        self.neg_iou_thresh = neg_iou_thresh\n",
    "        self.pos_ratio = pos_ratio\n",
    "\n",
    "    def __call__(self, bbox, anchor, img_size):\n",
    "\n",
    "        img_H, img_W = img_size\n",
    "\n",
    "        n_anchor = len(anchor)  # 9216\n",
    "        inside_index = get_inside_index(anchor, img_H, img_W)  # (2272,)\n",
    "        anchor = anchor[inside_index]  # (2272, 4)\n",
    "        argmax_ious, label = self._create_label(inside_index, anchor, bbox)\n",
    "\n",
    "        # compute bounding box regression targets\n",
    "        loc = bbox2loc(anchor, bbox[argmax_ious])  # (2272, 4)\n",
    "\n",
    "        # map up to original set of anchors\n",
    "        label = unmap(label, n_anchor, inside_index, fill=-1)  # (9216,)\n",
    "        loc = unmap(loc, n_anchor, inside_index, fill=0)  # (9216, 4)\n",
    "\n",
    "        return loc, label\n",
    "\n",
    "    def _create_label(self, inside_index, anchor, bbox):\n",
    "        # label) 1 :positive, 0 : negative, -1 : dont care\n",
    "        label = np.empty((len(inside_index),), dtype=np.int32)\n",
    "        label.fill(-1)\n",
    "\n",
    "        argmax_ious, max_ious, gt_argmax_ious = self._calc_ious(\n",
    "            anchor, bbox, inside_index\n",
    "        )\n",
    "\n",
    "        label[max_ious < self.neg_iou_thresh] = 0  # 0.3\n",
    "\n",
    "        # Í∞ÄÏû• iouÍ∞Ä ÌÅ∞ Í≤ÉÏùÄ positive label\n",
    "        label[gt_argmax_ious] = 1\n",
    "\n",
    "        # positive label\n",
    "        label[max_ious >= self.pos_iou_thresh] = 1  # 0.7\n",
    "\n",
    "        # subsample positive labels if we have too many\n",
    "        n_pos = int(self.pos_ratio * self.n_sample)\n",
    "        pos_index = np.where(label == 1)[0]\n",
    "        if len(pos_index) > n_pos:\n",
    "            disable_index = np.random.choice(\n",
    "                pos_index, size=(len(pos_index) - n_pos), replace=False\n",
    "            )\n",
    "            label[disable_index] = -1\n",
    "\n",
    "        # subsample negative labels if we have too many\n",
    "        n_neg = self.n_sample - np.sum(label == 1)\n",
    "        neg_index = np.where(label == 0)[0]\n",
    "        if len(neg_index) > n_neg:\n",
    "            disable_index = np.random.choice(\n",
    "                neg_index, size=(len(neg_index) - n_neg), replace=False\n",
    "            )\n",
    "            label[disable_index] = -1\n",
    "\n",
    "        return argmax_ious, label\n",
    "\n",
    "    def _calc_ious(self, anchor, bbox, inside_index):\n",
    "        # ious between the anchors and the gt boxes\n",
    "        ious = bbox_iou(anchor, bbox)\n",
    "        argmax_ious = ious.argmax(axis=1)\n",
    "        max_ious = ious[np.arange(len(inside_index)), argmax_ious]\n",
    "        gt_argmax_ious = ious.argmax(axis=0)\n",
    "        gt_max_ious = ious[gt_argmax_ious, np.arange(ious.shape[1])]\n",
    "        gt_argmax_ious = np.where(ious == gt_max_ious)[0]\n",
    "\n",
    "        return argmax_ious, max_ious, gt_argmax_ious"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a11881f",
   "metadata": {
    "id": "9a11881f"
   },
   "source": [
    "#### 2. positive, negative sampling\n",
    "RPNÏóêÏÑú NMSÎ•º Í±∞Ïπú roiÎì§ÏùÑ ground truthÏôÄÏùò iouÎ•º ÎπÑÍµê\\\n",
    "positive / negative sampling ÏàòÌñâ (Ï¥ù 128Í∞ú)\\\n",
    "sample roiÏôÄ gt_bboxÎ•º Ïù¥Ïö©Ìï¥ bbox regressionÏóêÏÑú regressionÌï¥ÏïºÌï† ground truth locÍ∞í(t_x, t_y, t_w, t_h)ÏùÑ Íµ¨Ìï®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6928b27",
   "metadata": {
    "id": "e6928b27"
   },
   "outputs": [],
   "source": [
    "class ProposalTargetCreator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_sample=128,\n",
    "        pos_ratio=0.25,\n",
    "        pos_iou_thresh=0.5,\n",
    "        neg_iou_thresh_hi=0.5,\n",
    "        neg_iou_thresh_lo=0.0,\n",
    "    ):\n",
    "        self.n_sample = n_sample\n",
    "        self.pos_ratio = pos_ratio\n",
    "        self.pos_iou_thresh = pos_iou_thresh  # positive iou threshold\n",
    "        self.neg_iou_thresh_hi = neg_iou_thresh_hi  # negitave iou threshold = (neg_iou_thresh_hi ~ neg_iou_thresh_lo)\n",
    "        self.neg_iou_thresh_lo = neg_iou_thresh_lo\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        roi,\n",
    "        bbox,\n",
    "        label,\n",
    "        loc_normalize_mean=(0.0, 0.0, 0.0, 0.0),\n",
    "        loc_normalize_std=(0.1, 0.1, 0.2, 0.2),\n",
    "    ):\n",
    "        n_bbox, _ = bbox.shape\n",
    "\n",
    "        roi = np.concatenate((roi, bbox), axis=0)\n",
    "\n",
    "        pos_roi_per_image = np.round(\n",
    "            self.n_sample * self.pos_ratio\n",
    "        )  # positive image Í∞ØÏàò = 32\n",
    "        iou = bbox_iou(roi, bbox)  # RoIÏôÄ bounding box IoU\n",
    "        gt_assignment = iou.argmax(axis=1)\n",
    "        max_iou = iou.max(axis=1)\n",
    "        gt_roi_label = (\n",
    "            label[gt_assignment] + 1\n",
    "        )  # class label [0, n_fg_class - 1] -> [1, n_fg_class].\n",
    "\n",
    "        # positive sample ÏÑ†ÌÉù (>= pos_iou_thresh IoU)\n",
    "        pos_index = np.where(max_iou >= self.pos_iou_thresh)[0]\n",
    "        pos_roi_per_this_image = int(min(pos_roi_per_image, pos_index.size))\n",
    "        if pos_index.size > 0:\n",
    "            pos_index = np.random.choice(\n",
    "                pos_index, size=pos_roi_per_this_image, replace=False\n",
    "            )\n",
    "\n",
    "        # Negative sample ÏÑ†ÌÉù [neg_iou_thresh_lo, neg_iou_thresh_hi)\n",
    "        neg_index = np.where(\n",
    "            (max_iou < self.neg_iou_thresh_hi) & (max_iou >= self.neg_iou_thresh_lo)\n",
    "        )[0]\n",
    "        neg_roi_per_this_image = self.n_sample - pos_roi_per_this_image\n",
    "        neg_roi_per_this_image = int(min(neg_roi_per_this_image, neg_index.size))\n",
    "        if neg_index.size > 0:\n",
    "            neg_index = np.random.choice(\n",
    "                neg_index, size=neg_roi_per_this_image, replace=False\n",
    "            )\n",
    "\n",
    "        # The indices that we're selecting (both positive and negative).\n",
    "        keep_index = np.append(pos_index, neg_index)\n",
    "        gt_roi_label = gt_roi_label[keep_index]\n",
    "        gt_roi_label[pos_roi_per_this_image:] = 0  # negative sampleÏùò label = 0\n",
    "        sample_roi = roi[keep_index]  # (128, 4)\n",
    "\n",
    "        # sample roiÏôÄ gt_bboxÎ•º Ïù¥Ïö©Ìï¥ bbox regressionÏóêÏÑú regressionÌï¥ÏïºÌï† ground truth locÍ∞í(t_x, t_y, t_w, t_h) Í≥ÑÏÇ∞\n",
    "        gt_roi_loc = bbox2loc(sample_roi, bbox[gt_assignment[keep_index]])  # (128, 4)\n",
    "        gt_roi_loc = (gt_roi_loc - np.array(loc_normalize_mean, np.float32)) / np.array(\n",
    "            loc_normalize_std, np.float32\n",
    "        )\n",
    "\n",
    "        return sample_roi, gt_roi_loc, gt_roi_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2da581",
   "metadata": {
    "id": "ec2da581"
   },
   "source": [
    "#### 3. Trainer Ï†ïÏùò\n",
    "training, loss Í≥ÑÏÇ∞, checkpoint Ï†ÄÏû• Î∞è Î∂àÎü¨Ïò§Í∏∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40ccd0b",
   "metadata": {
    "id": "d40ccd0b"
   },
   "outputs": [],
   "source": [
    "LossTuple = namedtuple(\n",
    "    \"LossTuple\",\n",
    "    [\"rpn_loc_loss\", \"rpn_cls_loss\", \"roi_loc_loss\", \"roi_cls_loss\", \"total_loss\"],\n",
    ")\n",
    "\n",
    "\n",
    "class FasterRCNNTrainer(nn.Module):\n",
    "    def __init__(self, faster_rcnn):\n",
    "        super(FasterRCNNTrainer, self).__init__()\n",
    "\n",
    "        self.faster_rcnn = faster_rcnn\n",
    "        self.rpn_sigma = rpn_sigma\n",
    "        self.roi_sigma = roi_sigma\n",
    "\n",
    "        # target creator create gt_bbox gt_label etc as training targets.\n",
    "        self.anchor_target_creator = AnchorTargetCreator()\n",
    "        self.proposal_target_creator = ProposalTargetCreator()\n",
    "\n",
    "        self.loc_normalize_mean = faster_rcnn.loc_normalize_mean\n",
    "        self.loc_normalize_std = faster_rcnn.loc_normalize_std\n",
    "\n",
    "        self.optimizer = self.faster_rcnn.get_optimizer()\n",
    "\n",
    "        # training ÏÉÅÌÉú Î≥¥Ïó¨Ï£ºÎäî ÏßÄÌëú\n",
    "        self.rpn_cm = ConfusionMeter(2)  # confusion matrix for classification\n",
    "        self.roi_cm = ConfusionMeter(11)  # confusion matrix for classification\n",
    "        self.meters = {\n",
    "            k: AverageValueMeter() for k in LossTuple._fields\n",
    "        }  # average loss\n",
    "\n",
    "    def forward(self, imgs, bboxes, labels, scale):\n",
    "        n = bboxes.shape[0]\n",
    "\n",
    "        if n != 1:\n",
    "            raise ValueError(\"Currently only batch size 1 is supported.\")\n",
    "\n",
    "        _, _, H, W = imgs.shape\n",
    "        img_size = (H, W)\n",
    "\n",
    "        # VGG (features extractor)\n",
    "        features = self.faster_rcnn.extractor(imgs)\n",
    "\n",
    "        # RPN (region proposal)\n",
    "        rpn_locs, rpn_scores, rois, roi_indices, anchor = self.faster_rcnn.rpn(\n",
    "            features, img_size, scale\n",
    "        )\n",
    "\n",
    "        # Since batch size is one, convert variables to singular form\n",
    "        bbox = bboxes[0]\n",
    "        label = labels[0]\n",
    "        rpn_score = rpn_scores[0]\n",
    "        rpn_loc = rpn_locs[0]\n",
    "        roi = rois\n",
    "\n",
    "        \"\"\"\n",
    "        sample roi =  rpnÏóêÏÑú nms Í±∞Ïπú 2000Í∞úÏùò roiÎì§ Ï§ë positive/negative ÎπÑÏú® Í≥†Î†§Ìï¥ ÏµúÏ¢Ö samplingÌïú roi\n",
    "        \"\"\"\n",
    "        sample_roi, gt_roi_loc, gt_roi_label = self.proposal_target_creator(\n",
    "            roi,\n",
    "            tonumpy(bbox),\n",
    "            tonumpy(label),\n",
    "            self.loc_normalize_mean,\n",
    "            self.loc_normalize_std,\n",
    "        )\n",
    "\n",
    "        # NOTE it's all zero because now it only support for batch=1 now\n",
    "        # Faster R-CNN head (prediction head)\n",
    "        sample_roi_index = torch.zeros(len(sample_roi))\n",
    "        roi_cls_loc, roi_score = self.faster_rcnn.head(\n",
    "            features, sample_roi, sample_roi_index\n",
    "        )\n",
    "\n",
    "        # ------------------ RPN losses -------------------#\n",
    "        gt_rpn_loc, gt_rpn_label = self.anchor_target_creator(\n",
    "            tonumpy(bbox), anchor, img_size\n",
    "        )\n",
    "        gt_rpn_label = totensor(gt_rpn_label).long()\n",
    "        gt_rpn_loc = totensor(gt_rpn_loc)\n",
    "\n",
    "        # rpn bounding box regression loss\n",
    "        rpn_loc_loss = _fast_rcnn_loc_loss(\n",
    "            rpn_loc, gt_rpn_loc, gt_rpn_label.data, self.rpn_sigma\n",
    "        )\n",
    "        # rpn classification loss\n",
    "        rpn_cls_loss = F.cross_entropy(rpn_score, gt_rpn_label.cuda(), ignore_index=-1)\n",
    "\n",
    "        _gt_rpn_label = gt_rpn_label[gt_rpn_label > -1]\n",
    "        _rpn_score = tonumpy(rpn_score)[tonumpy(gt_rpn_label) > -1]\n",
    "        self.rpn_cm.add(totensor(_rpn_score, False), _gt_rpn_label.data.long())\n",
    "\n",
    "        # ------------------ ROI losses (fast rcnn loss) -------------------#\n",
    "        n_sample = roi_cls_loc.shape[0]\n",
    "        roi_cls_loc = roi_cls_loc.view(n_sample, -1, 4)\n",
    "        roi_loc = roi_cls_loc[\n",
    "            torch.arange(0, n_sample).long().cuda(), totensor(gt_roi_label).long()\n",
    "        ]\n",
    "        gt_roi_label = totensor(gt_roi_label).long()\n",
    "        gt_roi_loc = totensor(gt_roi_loc)\n",
    "\n",
    "        # faster rcnn bounding box regression loss\n",
    "        roi_loc_loss = _fast_rcnn_loc_loss(\n",
    "            roi_loc.contiguous(), gt_roi_loc, gt_roi_label.data, self.roi_sigma\n",
    "        )\n",
    "\n",
    "        # faster rcnn classification loss\n",
    "        roi_cls_loss = nn.CrossEntropyLoss()(roi_score, gt_roi_label.cuda())\n",
    "\n",
    "        self.roi_cm.add(totensor(roi_score, False), gt_roi_label.data.long())\n",
    "\n",
    "        losses = [rpn_loc_loss, rpn_cls_loss, roi_loc_loss, roi_cls_loss]\n",
    "        losses = losses + [sum(losses)]  # total_loss == sum(losses)\n",
    "\n",
    "        return LossTuple(*losses)\n",
    "\n",
    "    # training\n",
    "    def train_step(self, imgs, bboxes, labels, scale):\n",
    "        self.optimizer.zero_grad()\n",
    "        losses = self.forward(imgs, bboxes, labels, scale)\n",
    "        losses.total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.update_meters(losses)\n",
    "        return losses\n",
    "\n",
    "    # checkpoint ÎßåÎì§Í∏∞\n",
    "    def save(self, save_optimizer=False, save_path=None):\n",
    "        save_dict = dict()\n",
    "\n",
    "        save_dict[\"model\"] = self.faster_rcnn.state_dict()\n",
    "\n",
    "        if save_optimizer:\n",
    "            save_dict[\"optimizer\"] = self.optimizer.state_dict()\n",
    "\n",
    "        if save_path is None:\n",
    "            save_path = \"./checkpoints/faster_rcnn_scratch_checkpoints.pth\"\n",
    "\n",
    "        save_dir = os.path.dirname(save_path)\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "        torch.save(save_dict, save_path)\n",
    "        return save_path\n",
    "\n",
    "    # checkpoint load\n",
    "    def load(\n",
    "        self,\n",
    "        path,\n",
    "        load_optimizer=True,\n",
    "        parse_opt=False,\n",
    "    ):\n",
    "        state_dict = torch.load(path)\n",
    "        if \"model\" in state_dict:\n",
    "            self.faster_rcnn.load_state_dict(state_dict[\"model\"])\n",
    "        else:  # legacy way, for backward compatibility\n",
    "            self.faster_rcnn.load_state_dict(state_dict)\n",
    "            return self\n",
    "        if \"optimizer\" in state_dict and load_optimizer:\n",
    "            self.optimizer.load_state_dict(state_dict[\"optimizer\"])\n",
    "        return self\n",
    "\n",
    "    def update_meters(self, losses):\n",
    "        loss_d = {k: scalar(v) for k, v in losses._asdict().items()}\n",
    "        for key, meter in self.meters.items():\n",
    "            meter.add(loss_d[key])\n",
    "\n",
    "    def reset_meters(self):\n",
    "        for key, meter in self.meters.items():\n",
    "            meter.reset()\n",
    "        self.roi_cm.reset()\n",
    "        self.rpn_cm.reset()\n",
    "\n",
    "    def get_meter_data(self):\n",
    "        return {k: v.value()[0] for k, v in self.meters.items()}\n",
    "\n",
    "\n",
    "def _smooth_l1_loss(x, t, in_weight, sigma):\n",
    "    sigma2 = sigma**2\n",
    "    diff = in_weight * (x - t)\n",
    "    abs_diff = diff.abs()\n",
    "    flag = (abs_diff.data < (1.0 / sigma2)).float()\n",
    "    y = flag * (sigma2 / 2.0) * (diff**2) + (1 - flag) * (abs_diff - 0.5 / sigma2)\n",
    "    return y.sum()\n",
    "\n",
    "\n",
    "def _fast_rcnn_loc_loss(pred_loc, gt_loc, gt_label, sigma):\n",
    "    # Localization loss Íµ¨Ìï† ÎïåÎäî positive exampleÏóê ÎåÄÌï¥ÏÑúÎßå Í≥ÑÏÇ∞\n",
    "    in_weight = torch.zeros(gt_loc.shape).cuda()\n",
    "    in_weight[(gt_label > 0).view(-1, 1).expand_as(in_weight).cuda()] = 1\n",
    "    loc_loss = _smooth_l1_loss(pred_loc, gt_loc, in_weight.detach(), sigma)\n",
    "    loc_loss /= (gt_label >= 0).sum().float()\n",
    "    return loc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b03eee6",
   "metadata": {
    "id": "2b03eee6"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9224c3",
   "metadata": {
    "id": "9c9224c3"
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Train dataset Î∂àÎü¨Ïò§Í∏∞\n",
    "    #     dataset = TrainDataset()\n",
    "    annotation = os.path.join(data_dir, \"train.json\")\n",
    "    dataset = TrainCustom(annotation, data_dir, transforms=True)\n",
    "    print(\"load data\")\n",
    "    dataloader = data_.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=1,  # only batch_size=1 support\n",
    "        shuffle=True,\n",
    "        pin_memory=False,\n",
    "        num_workers=0,\n",
    "    )\n",
    "\n",
    "    # faster rcnn Î∂àÎü¨Ïò§Í∏∞\n",
    "    faster_rcnn = FasterRCNNVGG16().cuda()\n",
    "    print(\"model construct completed\")\n",
    "\n",
    "    # faster rcnn trainer Î∂àÎü¨Ïò§Í∏∞\n",
    "    trainer = FasterRCNNTrainer(faster_rcnn).cuda()\n",
    "\n",
    "    # checkpoint load\n",
    "    if train_load_path:\n",
    "        trainer.load(train_load_path)\n",
    "        print(\"load pretrained model from %s\" % train_load_path)\n",
    "\n",
    "    lr_ = learning_rate\n",
    "    best_loss = 1000\n",
    "    for epoch in range(epochs):\n",
    "        trainer.reset_meters()\n",
    "        for ii, (img, bbox_, label_, scale) in enumerate(tqdm(dataloader)):\n",
    "\n",
    "            img, bbox, label = img.cuda().float(), bbox_.cuda(), label_.cuda()\n",
    "            trainer.train_step(img, bbox, label, float(scale))\n",
    "\n",
    "        losses = trainer.get_meter_data()\n",
    "        print(f\"Epoch #{epoch+1} loss: {losses}\")\n",
    "        if losses[\"total_loss\"] < best_loss:\n",
    "            trainer.save()\n",
    "\n",
    "        if epoch == 9:\n",
    "            trainer.faster_rcnn.scale_lr(lr_decay)\n",
    "            lr_ = lr_ * lr_decay\n",
    "\n",
    "        if epoch == 13:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d704420",
   "metadata": {
    "id": "1d704420"
   },
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0771b1d",
   "metadata": {
    "id": "b0771b1d"
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a67b27",
   "metadata": {
    "id": "43a67b27"
   },
   "outputs": [],
   "source": [
    "def eval(dataloader, faster_rcnn):\n",
    "    outputs = []\n",
    "    for ii, (imgs, sizes) in enumerate(tqdm(dataloader)):\n",
    "        sizes = [sizes[0][0].item(), sizes[1][0].item()]\n",
    "        pred_bboxes_, pred_labels_, pred_scores_ = faster_rcnn.predict(imgs, [sizes])\n",
    "        for out in range(len(pred_bboxes_)):\n",
    "            outputs.append(\n",
    "                {\n",
    "                    \"boxes\": pred_bboxes_[out],\n",
    "                    \"scores\": pred_scores_[out],\n",
    "                    \"labels\": pred_labels_[out],\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77514fc",
   "metadata": {
    "id": "c77514fc"
   },
   "outputs": [],
   "source": [
    "def inference():\n",
    "\n",
    "    # Test dataset Î∂àÎü¨Ïò§Í∏∞\n",
    "    #     testset = TestDataset()\n",
    "    annotation = os.path.join(data_dir, \"test.json\")\n",
    "    testset = TestCustom(annotation, data_dir)\n",
    "    test_dataloader = data_.DataLoader(\n",
    "        testset,\n",
    "        batch_size=1,  # only batch_size=1 support\n",
    "        num_workers=0,\n",
    "        shuffle=False,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "    # faster rcnn Î∂àÎü¨Ïò§Í∏∞\n",
    "    faster_rcnn = FasterRCNNVGG16().cuda()\n",
    "    state_dict = torch.load(inf_load_path)\n",
    "    if \"model\" in state_dict:\n",
    "        faster_rcnn.load_state_dict(state_dict[\"model\"])\n",
    "    print(\"load pretrained model from %s\" % inf_load_path)\n",
    "\n",
    "    # evaluation\n",
    "    outputs = eval(test_dataloader, faster_rcnn)\n",
    "    score_threshold = 0.05\n",
    "    prediction_strings = []\n",
    "    file_names = []\n",
    "\n",
    "    # submission file ÏûëÏÑ±\n",
    "    coco = COCO(os.path.join(data_dir, \"test.json\"))\n",
    "    for i, output in enumerate(outputs):\n",
    "        prediction_string = \"\"\n",
    "        image_info = coco.loadImgs(coco.getImgIds(imgIds=i))[0]\n",
    "        for box, score, label in zip(\n",
    "            output[\"boxes\"], output[\"scores\"], output[\"labels\"]\n",
    "        ):\n",
    "            if score > score_threshold:\n",
    "                prediction_string += (\n",
    "                    str(label)\n",
    "                    + \" \"\n",
    "                    + str(score)\n",
    "                    + \" \"\n",
    "                    + str(box[1])\n",
    "                    + \" \"\n",
    "                    + str(box[0])\n",
    "                    + \" \"\n",
    "                    + str(box[3])\n",
    "                    + \" \"\n",
    "                    + str(box[2])\n",
    "                    + \" \"\n",
    "                )\n",
    "        prediction_strings.append(prediction_string)\n",
    "        file_names.append(image_info[\"file_name\"])\n",
    "    submission = pd.DataFrame()\n",
    "    submission[\"PredictionString\"] = prediction_strings\n",
    "    submission[\"image_id\"] = file_names\n",
    "    submission.to_csv(\"./faster_rcnn_scratch_submission.csv\", index=False)\n",
    "\n",
    "    print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b16e50",
   "metadata": {
    "id": "06b16e50"
   },
   "outputs": [],
   "source": [
    "inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f8b59f",
   "metadata": {
    "id": "c4f8b59f"
   },
   "source": [
    "# Reference\n",
    "https://github.com/chenyuntc/simple-faster-rcnn-pytorch \\\n",
    "https://github.com/shkim960520/faster-rcnn-for-studying "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2134f28e",
   "metadata": {
    "id": "2134f28e"
   },
   "source": [
    "###**ÏΩòÌÖêÏ∏† ÎùºÏù¥ÏÑ†Ïä§**\n",
    "\n",
    "<font color='red'><b>**WARNING**</b></font> : **Î≥∏ ÍµêÏú° ÏΩòÌÖêÏ∏†Ïùò ÏßÄÏãùÏû¨ÏÇ∞Í∂åÏùÄ Ïû¨Îã®Î≤ïÏù∏ ÎÑ§Ïù¥Î≤ÑÏª§ÎÑ•Ìä∏Ïóê Í∑ÄÏÜçÎê©ÎãàÎã§. Î≥∏ ÏΩòÌÖêÏ∏†Î•º Ïñ¥Îñ†Ìïú Í≤ΩÎ°úÎ°úÎì† Ïô∏Î∂ÄÎ°ú Ïú†Ï∂ú Î∞è ÏàòÏ†ïÌïòÎäî ÌñâÏúÑÎ•º ÏóÑÍ≤©Ìûà Í∏àÌï©ÎãàÎã§.** Îã§Îßå, ÎπÑÏòÅÎ¶¨Ï†Å ÍµêÏú° Î∞è Ïó∞Íµ¨ÌôúÎèôÏóê ÌïúÏ†ïÎêòÏñ¥ ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏúºÎÇò Ïû¨Îã®Ïùò ÌóàÎùΩÏùÑ Î∞õÏïÑÏïº Ìï©ÎãàÎã§. Ïù¥Î•º ÏúÑÎ∞òÌïòÎäî Í≤ΩÏö∞, Í¥ÄÎ†® Î≤ïÎ•†Ïóê Îî∞Îùº Ï±ÖÏûÑÏùÑ Ïßà Ïàò ÏûàÏäµÎãàÎã§.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}